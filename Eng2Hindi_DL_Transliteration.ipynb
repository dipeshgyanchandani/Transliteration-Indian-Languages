{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GokulNC/NLP-Exercises/blob/master/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IOI4rlrgxPI-"
      },
      "source": [
        "## Setting up the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1GqTjeV47m4B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Instantiates the device to be used as GPU/CPU based on availability\n",
        "device_gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Visualization tools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import random\n",
        "# from torchtext.data import Field, BucketIterator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KpuvHS0mxwCd"
      },
      "source": [
        "## Data Management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eYrAa5laSptM"
      },
      "source": [
        "### Alphabets Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "colab_type": "code",
        "id": "-a04ZKx7Sh-J",
        "outputId": "8c42bbc0-3890-4b86-f1c8-d4879e47afe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'-PAD-': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26, '-SOS-': 27, '-EOS-': 28}\n"
          ]
        }
      ],
      "source": [
        "eng_alphabets = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "pad_char = '-PAD-'\n",
        "start_char = '-SOS-'\n",
        "end_char = '-EOS-'\n",
        "\n",
        "eng_alpha2index = {pad_char: 0}\n",
        "for index, alpha in enumerate(eng_alphabets):\n",
        "    eng_alpha2index[alpha] = index+1\n",
        "\n",
        "eng_alpha2index[start_char] = 27\n",
        "eng_alpha2index[end_char] = 28\n",
        "print(eng_alpha2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "colab_type": "code",
        "id": "cPSZsy1kXd9w",
        "outputId": "474a43f1-3617-4aed-aec5-fc758ad41b1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'-PAD-': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128, '-SOS-': 129, '-EOS-': 130}\n"
          ]
        }
      ],
      "source": [
        "# Hindi Unicode Hex Range: https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)\n",
        "hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
        "hindi_alphabet_size = len(hindi_alphabets)\n",
        "\n",
        "hindi_alpha2index = {pad_char: 0}\n",
        "hindi_index2alpha = {0: pad_char}\n",
        "for index, alpha in enumerate(hindi_alphabets):\n",
        "    hindi_alpha2index[alpha] = index+1\n",
        "    hindi_index2alpha[index+1] = alpha\n",
        "\n",
        "hindi_alpha2index[start_char] = 129\n",
        "hindi_alpha2index[end_char] = 130\n",
        "hindi_index2alpha[129] = start_char\n",
        "hindi_index2alpha[130] = end_char\n",
        "print(hindi_alpha2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ऀएकँ-PAD--PAD-\n"
          ]
        }
      ],
      "source": [
        "array = [[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
        "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
        "        [ 6, 10, 49, 16, 39, 58, 37, 45, 21, 10, 43, 37, 43, 49, 54, 49, 22, 24,\n",
        "         24, 22, 58, 43, 22, 11, 57, 16, 29, 16, 48, 16, 25, 39],\n",
        "        [45, 37, 76, 22, 64, 32, 37, 57, 49, 47, 49, 45, 49, 58, 22, 67, 49, 63,\n",
        "         48, 66, 66, 49, 49, 43, 47, 22, 49, 22, 58, 22, 32, 72],\n",
        "        [ 2, 49, 29,  2, 41,  2, 78,  2,  2, 78,  2,  2,  2, 65, 65, 43,  2, 16,\n",
        "         63, 28, 16,  2,  2, 49, 48,  2, 63,  2,  2,  2,  2, 49],\n",
        "        [ 0, 65,  2,  0,  2,  0, 54,  0,  0, 49,  0,  0,  0,  2, 51,  2,  0,  2,\n",
        "          2,  2,  2,  0,  0,  2,  2,  0,  2,  0,  0,  0,  0,  2],\n",
        "        [ 0,  2,  0,  0,  0,  0,  2,  0,  0,  2,  0,  0,  0,  0,  2,  0,  0,  0,\n",
        "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]\n",
        "\n",
        "testword = ''\n",
        "for a in array:\n",
        "    testword += hindi_index2alpha[a[3]]\n",
        "\n",
        "print(testword)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M9iH3ZvyOeNa"
      },
      "source": [
        "### Downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "colab_type": "code",
        "id": "76HwbPfrOg_a",
        "outputId": "12756009-32ab-452c-8156-0754bc2ec9d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: wget\n",
            "zsh:1: command not found: wget\n"
          ]
        }
      ],
      "source": [
        "## Train data file\n",
        "!wget -nc -q --show-progress https://github.com/GokulNC/NLP-Exercises/raw/master/Transliteration-Indian-Languages/Original-NEWS2012-data/Training/NEWS2012-Training-EnHi-13937.xml\n",
        "## Test data file\n",
        "!wget -nc -q --show-progress https://raw.githubusercontent.com/GokulNC/NLP-Exercises/master/Transliteration-Indian-Languages/Original-NEWS2012-data/Ref/NEWS2012-Ref-EnHi-1000.xml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SSw1SMZmx9A3"
      },
      "source": [
        "### Helper functions for data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OcS6ByndOxrC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "non_eng_letters_regex = re.compile('[^a-zA-Z ]')\n",
        "\n",
        "# Returns a list of English words in the given line,\n",
        "# containing only upper-case letters\n",
        "def cleanEnglishVocab(line):\n",
        "    line = line.replace('-', ' ').replace(',', ' ').upper()\n",
        "    # Remove all non-letters\n",
        "    line = non_eng_letters_regex.sub('', line)\n",
        "    return line.split()\n",
        "\n",
        "# Returns a list of Hindi words in the given line,\n",
        "# containing only Hindi characters\n",
        "def cleanHindiVocab(line):\n",
        "    line = line.replace('-', ' ').replace(',', ' ')\n",
        "    # Remove all non-Hindi chars except space\n",
        "    cleaned_line = ''\n",
        "    for char in line:\n",
        "        if char in hindi_alpha2index or char == ' ':\n",
        "            cleaned_line += char\n",
        "    \n",
        "    return cleaned_line.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ob3F9Dh4PChB"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "colab_type": "code",
        "id": "KGSeoMGg0FTy",
        "outputId": "e0fb18cd-e3ec-4057-e2e8-7a5f907f01fb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "class TransliterationDataLoader(Dataset):\n",
        "    def __init__(self, filename):\n",
        "        self.eng_words, self.hindi_words = self.readXmlDataset(filename, cleanHindiVocab)\n",
        "        self.shuffle_indices = list(range(len(self.eng_words)))\n",
        "        random.shuffle(self.shuffle_indices)\n",
        "        self.shuffle_start_index = 0\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.eng_words)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.eng_words[idx], self.hindi_words[idx]\n",
        "    \n",
        "    def readXmlDataset(self, filename, lang_vocab_cleaner):\n",
        "        transliterationCorpus = ET.parse(filename).getroot()\n",
        "        lang1_words = []\n",
        "        lang2_words = []\n",
        "\n",
        "        for line in transliterationCorpus:\n",
        "            wordlist1 = cleanEnglishVocab(line[0].text)\n",
        "            wordlist2 = lang_vocab_cleaner(line[1].text)\n",
        "\n",
        "            # Skip noisy data\n",
        "            if len(wordlist1) != len(wordlist2):\n",
        "                print('Skipping: ', line[0].text, ' - ', line[1].text)\n",
        "                continue\n",
        "\n",
        "            for word in wordlist1:\n",
        "                lang1_words.append(word)\n",
        "            for word in wordlist2:\n",
        "                lang2_words.append(word)\n",
        "\n",
        "        return lang1_words, lang2_words\n",
        "    \n",
        "    def get_random_sample(self):\n",
        "        return self.__getitem__(np.random.randint(len(self.eng_words)))\n",
        "    \n",
        "    def get_batch_from_array(self, batch_size, array):\n",
        "        end = self.shuffle_start_index + batch_size\n",
        "        batch = []\n",
        "        if end >= len(self.eng_words):\n",
        "            batch = [array[i] for i in self.shuffle_indices[0:end%len(self.eng_words)]]\n",
        "            end = len(self.eng_words)\n",
        "        return batch + [array[i] for i in self.shuffle_indices[self.shuffle_start_index : end]]\n",
        "    \n",
        "    def get_batch(self, batch_size, postprocess = True):\n",
        "        eng_batch = self.get_batch_from_array(batch_size, self.eng_words)\n",
        "        hindi_batch = self.get_batch_from_array(batch_size, self.hindi_words)\n",
        "        self.shuffle_start_index += batch_size + 1\n",
        "        \n",
        "        # Reshuffle if 1 epoch is complete\n",
        "        if self.shuffle_start_index >= len(self.eng_words):\n",
        "            random.shuffle(self.shuffle_indices)\n",
        "            self.shuffle_start_index = 0\n",
        "            \n",
        "        return eng_batch, hindi_batch\n",
        "    \n",
        "train_data = TransliterationDataLoader('Original-NEWS2012-data/Training/NEWS2012-Training-EnHi-13937.xml')\n",
        "test_data = TransliterationDataLoader('Original-NEWS2012-data/Training/NEWS2012-Training-EnHi-13937.xml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7l-iaCVdx5Ez"
      },
      "source": [
        "### Basic Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "colab_type": "code",
        "id": "IjY06ghEx76b",
        "outputId": "00a65f73-ea9d-4842-bf0e-e56d3f421119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Set Size:\t 20641\n",
            "Test Set Size:\t 20641\n",
            "\n",
            "Sample data from train-set:\n",
            "BABUGARH - बाबूगढ़\n",
            "CROWNHILL - क्राउनहिल\n",
            "GONDA - गोंडा\n",
            "SAMASTIPUR - समस्तीपुर\n",
            "MILLIE - मिली\n",
            "MEIN - में\n",
            "ALLAHABAD - इलाहाबाद\n",
            "RATNAVALI - रत्नावली\n",
            "SONI - सोनी\n",
            "ROBINSON - रॉबिंसन\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Set Size:\\t\", len(train_data))\n",
        "print(\"Test Set Size:\\t\", len(test_data))\n",
        "\n",
        "print('\\nSample data from train-set:')\n",
        "for i in range(10):\n",
        "    eng, hindi = train_data.get_random_sample()\n",
        "    print(eng + ' - ' + hindi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KpDP1_KYZIkv"
      },
      "source": [
        "### Encoding the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "colab_type": "code",
        "id": "JE3at5C7Sy5F",
        "outputId": "eae00864-fd69-4542-9ac6-828f868d5fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPEN tensor([[27],\n",
            "        [15],\n",
            "        [16],\n",
            "        [ 5],\n",
            "        [14],\n",
            "        [28]])\n",
            "ओपन tensor([[129],\n",
            "        [ 20],\n",
            "        [ 43],\n",
            "        [ 41],\n",
            "        [130]])\n"
          ]
        }
      ],
      "source": [
        "def word_rep(word, letter2index, device = 'cpu'):\n",
        "    rep = torch.zeros(len(word)+1, 1, len(letter2index)).to(device)\n",
        "    for letter_index, letter in enumerate(word):\n",
        "        pos = letter2index[letter]\n",
        "        rep[letter_index][0][pos] = 1\n",
        "    pad_pos = letter2index[pad_char]\n",
        "    rep[letter_index+1][0][pad_pos] = 1\n",
        "    return rep\n",
        "\n",
        "\n",
        "def input_rep(word, letter2index, device = 'cpu'):\n",
        "    rep = torch.zeros([len(word)+2, 1], dtype=torch.long).to(device)\n",
        "    letter_index = 0\n",
        "    rep[letter_index][0] = letter2index[start_char]\n",
        "    for _, letter in enumerate(word):\n",
        "        pos = letter2index[letter]\n",
        "        letter_index +=1\n",
        "        rep[letter_index][0] = pos\n",
        "    rep[-1][0] = letter2index[end_char]\n",
        "    return rep\n",
        "\n",
        "\n",
        "def gt_rep(word, letter2index, device = 'cpu'):\n",
        "    gt_rep = torch.zeros([len(word)+2, 1], dtype=torch.long).to(device)\n",
        "    letter_index = 0\n",
        "    gt_rep[letter_index][0] = letter2index[start_char]\n",
        "    for _, letter in enumerate(word):\n",
        "        pos = letter2index[letter]\n",
        "        letter_index +=1\n",
        "        gt_rep[letter_index][0] = pos\n",
        "    gt_rep[-1][0] = letter2index[end_char]\n",
        "    return gt_rep\n",
        "\n",
        "eng, hindi = train_data.get_random_sample()\n",
        "eng_rep = input_rep(eng, eng_alpha2index)\n",
        "print(eng, eng_rep)\n",
        "\n",
        "hindi_gt = gt_rep(hindi, hindi_alpha2index)\n",
        "print(hindi, hindi_gt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GrC3tSnm4rUk"
      },
      "source": [
        "## Network Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D4OgdZ_DVVC5"
      },
      "source": [
        "### Encoder-Decoder (using GRU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Shape of x is seq_length, N where N is batch size\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        print(\"Encoder Model: \")\n",
        "        print(\"X shape: \", x.shape)\n",
        "        print(\"Embedding shape: \", embedding.shape)\n",
        "        # embedding shape: (seq_length, N, embedding_size)\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedding)\n",
        "        print(\"outputs: \", outputs.shape)\n",
        "        print(\"hidden: \", hidden.shape)\n",
        "        print(\"cell: \", cell.shape)\n",
        "        \n",
        "\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_Size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x, hidden, cell):\n",
        "        # shape of x is (N) but we want (1, N)\n",
        "        x = x.unsqueeze(0)\n",
        "        print(\"Decoder Model: \")\n",
        "        print(\"x: \", x.shape)\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        #embedding shape: 1, N, embedding_size\n",
        "        print(\"embedding: \", embedding.shape)\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "        #shape of outputs: (1, N, hidden_size)\n",
        "        print(\"outputs: \", outputs.shape)\n",
        "        print(\"hidden: \", hidden.shape)\n",
        "        print(\"cell: \", cell.shape)\n",
        "\n",
        "        predictions = self.fc(outputs)\n",
        "        #shape of predictions : (1, N, length_of_vocab)\n",
        "        print(\"predictions: \", predictions.shape)\n",
        "\n",
        "        predictions = predictions.squeeze(0)\n",
        "        print(\"predictions: \", predictions.shape)\n",
        "\n",
        "        return predictions, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq,self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "\n",
        "        target_vocab_size = len(hindi_alpha2index)\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device_gpu)\n",
        "        \n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        x = target[0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "\n",
        "            outputs[t] = output\n",
        "            # ouptut is N, hindi_vocab_size\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "#training hyperparameters\n",
        "\n",
        "num_epochs = 17000\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "\n",
        "load_model = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder Model: \n",
            "X shape:  torch.Size([9, 1])\n",
            "Embedding shape:  torch.Size([9, 1, 300])\n",
            "outputs:  torch.Size([9, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([9, 1])\n",
            "Embedding shape:  torch.Size([9, 1, 300])\n",
            "outputs:  torch.Size([9, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([10, 1])\n",
            "Embedding shape:  torch.Size([10, 1, 300])\n",
            "outputs:  torch.Size([10, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([6, 1])\n",
            "Embedding shape:  torch.Size([6, 1, 300])\n",
            "outputs:  torch.Size([6, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([6, 1])\n",
            "Embedding shape:  torch.Size([6, 1, 300])\n",
            "outputs:  torch.Size([6, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([6, 1])\n",
            "Embedding shape:  torch.Size([6, 1, 300])\n",
            "outputs:  torch.Size([6, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([13, 1])\n",
            "Embedding shape:  torch.Size([13, 1, 300])\n",
            "outputs:  torch.Size([13, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([5, 1])\n",
            "Embedding shape:  torch.Size([5, 1, 300])\n",
            "outputs:  torch.Size([5, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([8, 1])\n",
            "Embedding shape:  torch.Size([8, 1, 300])\n",
            "outputs:  torch.Size([8, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([6, 1])\n",
            "Embedding shape:  torch.Size([6, 1, 300])\n",
            "outputs:  torch.Size([6, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([6, 1])\n",
            "Embedding shape:  torch.Size([6, 1, 300])\n",
            "outputs:  torch.Size([6, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([9, 1])\n",
            "Embedding shape:  torch.Size([9, 1, 300])\n",
            "outputs:  torch.Size([9, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([10, 1])\n",
            "Embedding shape:  torch.Size([10, 1, 300])\n",
            "outputs:  torch.Size([10, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([11, 1])\n",
            "Embedding shape:  torch.Size([11, 1, 300])\n",
            "outputs:  torch.Size([11, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([10, 1])\n",
            "Embedding shape:  torch.Size([10, 1, 300])\n",
            "outputs:  torch.Size([10, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([13, 1])\n",
            "Embedding shape:  torch.Size([13, 1, 300])\n",
            "outputs:  torch.Size([13, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([8, 1])\n",
            "Embedding shape:  torch.Size([8, 1, 300])\n",
            "outputs:  torch.Size([8, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([6, 1])\n",
            "Embedding shape:  torch.Size([6, 1, 300])\n",
            "outputs:  torch.Size([6, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([11, 1])\n",
            "Embedding shape:  torch.Size([11, 1, 300])\n",
            "outputs:  torch.Size([11, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([10, 1])\n",
            "Embedding shape:  torch.Size([10, 1, 300])\n",
            "outputs:  torch.Size([10, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([10, 1])\n",
            "Embedding shape:  torch.Size([10, 1, 300])\n",
            "outputs:  torch.Size([10, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([10, 1])\n",
            "Embedding shape:  torch.Size([10, 1, 300])\n",
            "outputs:  torch.Size([10, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([8, 1])\n",
            "Embedding shape:  torch.Size([8, 1, 300])\n",
            "outputs:  torch.Size([8, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([12, 1])\n",
            "Embedding shape:  torch.Size([12, 1, 300])\n",
            "outputs:  torch.Size([12, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([9, 1])\n",
            "Embedding shape:  torch.Size([9, 1, 300])\n",
            "outputs:  torch.Size([9, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([6, 1])\n",
            "Embedding shape:  torch.Size([6, 1, 300])\n",
            "outputs:  torch.Size([6, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([8, 1])\n",
            "Embedding shape:  torch.Size([8, 1, 300])\n",
            "outputs:  torch.Size([8, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([8, 1])\n",
            "Embedding shape:  torch.Size([8, 1, 300])\n",
            "outputs:  torch.Size([8, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([8, 1])\n",
            "Embedding shape:  torch.Size([8, 1, 300])\n",
            "outputs:  torch.Size([8, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([8, 1])\n",
            "Embedding shape:  torch.Size([8, 1, 300])\n",
            "outputs:  torch.Size([8, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([8, 1])\n",
            "Embedding shape:  torch.Size([8, 1, 300])\n",
            "outputs:  torch.Size([8, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([10, 1])\n",
            "Embedding shape:  torch.Size([10, 1, 300])\n",
            "outputs:  torch.Size([10, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([9, 1])\n",
            "Embedding shape:  torch.Size([9, 1, 300])\n",
            "outputs:  torch.Size([9, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([12, 1])\n",
            "Embedding shape:  torch.Size([12, 1, 300])\n",
            "outputs:  torch.Size([12, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([8, 1])\n",
            "Embedding shape:  torch.Size([8, 1, 300])\n",
            "outputs:  torch.Size([8, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([8, 1])\n",
            "Embedding shape:  torch.Size([8, 1, 300])\n",
            "outputs:  torch.Size([8, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([8, 1])\n",
            "Embedding shape:  torch.Size([8, 1, 300])\n",
            "outputs:  torch.Size([8, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([12, 1])\n",
            "Embedding shape:  torch.Size([12, 1, 300])\n",
            "outputs:  torch.Size([12, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([10, 1])\n",
            "Embedding shape:  torch.Size([10, 1, 300])\n",
            "outputs:  torch.Size([10, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([4, 1])\n",
            "Embedding shape:  torch.Size([4, 1, 300])\n",
            "outputs:  torch.Size([4, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([9, 1])\n",
            "Embedding shape:  torch.Size([9, 1, 300])\n",
            "outputs:  torch.Size([9, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([9, 1])\n",
            "Embedding shape:  torch.Size([9, 1, 300])\n",
            "outputs:  torch.Size([9, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([9, 1])\n",
            "Embedding shape:  torch.Size([9, 1, 300])\n",
            "outputs:  torch.Size([9, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([7, 1])\n",
            "Embedding shape:  torch.Size([7, 1, 300])\n",
            "outputs:  torch.Size([7, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([9, 1])\n",
            "Embedding shape:  torch.Size([9, 1, 300])\n",
            "outputs:  torch.Size([9, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([10, 1])\n",
            "Embedding shape:  torch.Size([10, 1, 300])\n",
            "outputs:  torch.Size([10, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Encoder Model: \n",
            "X shape:  torch.Size([8, 1])\n",
            "Embedding shape:  torch.Size([8, 1, 300])\n",
            "outputs:  torch.Size([8, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([1, 1, 1024])\n",
            "cell:  torch.Size([1, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb Cell 25\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X34sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X34sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X34sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X34sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), max_norm\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X34sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/Documents/ML/my-venv1/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/ML/my-venv1/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "input_size_encoder = len(eng_alpha2index)\n",
        "input_size_decoder = len(hindi_alpha2index)\n",
        "output_size = len(hindi_alpha2index)\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "\n",
        "hidden_size = 1024\n",
        "num_layers = 1 # 2 for without attention.\n",
        "encoder_dropout = 0.5\n",
        "decoder_dropout = 0.5\n",
        "\n",
        "#Tensorboard\n",
        "\n",
        "# writer = SummaryWriter(f'runs/Loss_plot')\n",
        "step = 0\n",
        "# train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "#     (train_data, test_data),\n",
        "#     batch_size = batch_size,\n",
        "#     sort_within_batch = True,\n",
        "#     sort_key = lambda x: len(x.src),\n",
        "#     device=device_gpu\n",
        "# )\n",
        "\n",
        "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, encoder_dropout).to(device_gpu)\n",
        "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size,output_size, num_layers, decoder_dropout).to(device_gpu)\n",
        "\n",
        "model = Seq2Seq(encoder_net, decoder_net).to(device_gpu)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "pad_idx = eng_alpha2index['-PAD-']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    eng_batch, hindi_batch = train_data.get_batch(batch_size)\n",
        "    input = input_rep(eng_batch[i], eng_alpha2index, device_gpu)\n",
        "    gt = gt_rep(hindi_batch[i], hindi_alpha2index, device_gpu)\n",
        "    inp_data = input.to(torch.int64)\n",
        "    target = gt\n",
        "\n",
        "    output = model(inp_data, target)\n",
        "    #output = tar_len, batch_size, output_size\n",
        "\n",
        "    #(N, 10) and terget would be (N)\n",
        "\n",
        "    output = output.reshape(-1, output.shape[2])\n",
        "    target = target.reshape(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "    optimizer.step()\n",
        "\n",
        "    step +=1\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_sentence(model, english, hindi, device, max_length=50):\n",
        "    # print(sentence)\n",
        "\n",
        "    input = input_rep(english, eng_alpha2index, device_gpu)\n",
        "    gt = gt_rep(hindi, hindi_alpha2index, device_gpu)\n",
        "\n",
        "    # # Convert to Tensor\n",
        "    # sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(input)\n",
        "\n",
        "    outputs = [hindi_alpha2index[\"-PAD-\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == hindi_alpha2index[\"-EOS-\"]:\n",
        "            break\n",
        "    print(outputs)\n",
        "    translated_sentence = [hindi_index2alpha[idx] for idx in outputs]\n",
        "    hindi_output = ''\n",
        "    for char in outputs:\n",
        "        hindi_output += hindi_index2alpha[char]\n",
        "\n",
        "    # remove start token\n",
        "    return translated_sentence, hindi_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder Model: \n",
            "X shape:  torch.Size([6, 1])\n",
            "Embedding shape:  torch.Size([6, 1, 300])\n",
            "outputs:  torch.Size([6, 1, 1024])\n",
            "hidden:  torch.Size([2, 1, 1024])\n",
            "cell:  torch.Size([2, 1, 1024])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([2, 1, 1024])\n",
            "cell:  torch.Size([2, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([2, 1, 1024])\n",
            "cell:  torch.Size([2, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([2, 1, 1024])\n",
            "cell:  torch.Size([2, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "Decoder Model: \n",
            "x:  torch.Size([1, 1])\n",
            "embedding:  torch.Size([1, 1, 300])\n",
            "outputs:  torch.Size([1, 1, 1024])\n",
            "hidden:  torch.Size([2, 1, 1024])\n",
            "cell:  torch.Size([2, 1, 1024])\n",
            "predictions:  torch.Size([1, 1, 131])\n",
            "predictions:  torch.Size([1, 131])\n",
            "[0, 51, 76, 54, 130]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(['-PAD-', 'ल', 'ो', 'व', '-EOS-'], '-PAD-लोव-EOS-')"
            ]
          },
          "execution_count": 214,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "english = 'LOVE'\n",
        "hindi = 'इडेलिया'\n",
        "device = device_gpu\n",
        "\n",
        "translate_sentence(model, english, hindi, device, max_length=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def infer(net, word, char_limit, device = 'cpu'):\n",
        "    input = input_rep(word, eng_alpha2index, device)\n",
        "    return net(input, char_limit)\n",
        "\n",
        "def test(net, word, device = 'cpu'):\n",
        "    net = net.eval().to(device)\n",
        "    outputs = infer(net, word, 30, device)\n",
        "    hindi_output = ''\n",
        "    for out in outputs:\n",
        "        val, indices = out.topk(1)\n",
        "        index = indices.tolist()[0][0]\n",
        "        if index == 0:\n",
        "            break\n",
        "        hindi_char = hindi_alphabets[index+1]\n",
        "        hindi_output += hindi_char\n",
        "    print(word + ' - ' + hindi_output)\n",
        "    return hindi_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test(model, \u001b[39m'\u001b[39;49m\u001b[39mDIKHSHA\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "\u001b[1;32m/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb Cell 28\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#Y104sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest\u001b[39m(net, word, device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#Y104sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     net \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39meval()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#Y104sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     outputs \u001b[39m=\u001b[39m infer(net, word, \u001b[39m30\u001b[39;49m, device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#Y104sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     hindi_output \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#Y104sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m outputs:\n",
            "\u001b[1;32m/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb Cell 28\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minfer\u001b[39m(net, word, char_limit, device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#Y104sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m input_rep(word, eng_alpha2index, device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#Y104sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m net(\u001b[39minput\u001b[39;49m, char_limit)\n",
            "File \u001b[0;32m~/Documents/ML/my-venv1/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/ML/my-venv1/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;32m/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb Cell 28\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#Y104sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, source, target, teacher_force_ratio\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#Y104sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     batch_size \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#Y104sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     target_len \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#Y104sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     target_vocab_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(hindi_alpha2index)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#Y104sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(target_len, batch_size, target_vocab_size)\u001b[39m.\u001b[39mto(device_gpu)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "test(model, 'DIKHSHA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Attention:\n",
        "\n",
        "class EncoderAttn(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "        super(EncoderAttn, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=True)\n",
        "\n",
        "        self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (seq_length, N) where N is batch size\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # embedding shape: (seq_length, N, embedding_size)\n",
        "\n",
        "        encoder_states, (hidden, cell) = self.rnn(embedding)\n",
        "        # outputs shape: (seq_length, N, hidden_size)\n",
        "\n",
        "        # Use forward, backward cells and hidden through a linear layer\n",
        "        # so that it can be input to the decoder which is not bidirectional\n",
        "        # Also using index slicing ([idx:idx+1]) to keep the dimension\n",
        "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
        "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
        "\n",
        "        return encoder_states, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class DecoderAttn(nn.Module):\n",
        "#     def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
        "#         super(DecoderAttn, self).__init__()\n",
        "#         self.hidden_Size = hidden_size\n",
        "#         self.num_layers = num_layers\n",
        "#         self.dropout = nn.Dropout(p)\n",
        "#         self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "#         self.rnn = nn.LSTM(hidden_size*2 + embedding_size, hidden_size, num_layers, dropout=p)\n",
        "#         self.energy = nn.Linear(hidden_size * 3, 1)\n",
        "#         self.softmax = nn.Softmax(dim=0)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.fc = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "#     def forward(self, x, encoder_state, hidden, cell):\n",
        "#         # shape of x is (N) but we want (1, N)\n",
        "#         x = x.unsqueeze(0)\n",
        "\n",
        "#         embedding = self.dropout(self.embedding(x))\n",
        "#         #embedding shape: 1, N, embedding_size\n",
        "#         sequence_length = encoder_state.shape[0]\n",
        "#         h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
        "#         energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_state), dim=2)))\n",
        "#         attention = self.softmax(energy)\n",
        "#         # attention: seq_len, N, 1\n",
        "#         #Encoder states: (N, )\n",
        "#         attention = attention.permute(1, 2, 0)\n",
        "#         #(N, 1, seq_len)\n",
        "#         encoder_states = encoder_state.permute(1, 0, 2)\n",
        "        \n",
        "#         context_vector = torch.bmm(attention, encoder_states).permute(1, 0, 2)\n",
        "#         rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
        "        \n",
        "#         outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "#         #shape of outputs: (1, N, hidden_size)\n",
        "#         # print(\"outputs: \", outputs.shape)\n",
        "#         # print(\"hidden: \", hidden.shape)\n",
        "#         # print(\"cell: \", cell.shape)\n",
        "\n",
        "#         predictions = self.fc(outputs)\n",
        "#         # #shape of predictions : (1, N, length_of_vocab)\n",
        "#         # print(\"predictions: \", predictions.shape)\n",
        "\n",
        "#         predictions = predictions.squeeze(0)\n",
        "#         # print(\"predictions: \", predictions.shape)\n",
        "#         return predictions, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderAttn(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
        "    ):\n",
        "        super(DecoderAttn, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(hidden_size * 2 + embedding_size, hidden_size, num_layers)\n",
        "\n",
        "        self.energy = nn.Linear(hidden_size * 3, 1)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, encoder_states, hidden, cell):\n",
        "        x = x.unsqueeze(0)\n",
        "        # x: (1, N) where N is the batch size\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # embedding shape: (1, N, embedding_size)\n",
        "\n",
        "        sequence_length = encoder_states.shape[0]\n",
        "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
        "        # h_reshaped: (seq_length, N, hidden_size*2)\n",
        "\n",
        "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
        "        # energy: (seq_length, N, 1)\n",
        "\n",
        "        attention = self.softmax(energy)\n",
        "        # attention: (seq_length, N, 1)\n",
        "\n",
        "        # attention: (seq_length, N, 1), snk\n",
        "        # encoder_states: (seq_length, N, hidden_size*2), snl\n",
        "        # we want context_vector: (1, N, hidden_size*2), i.e knl\n",
        "        context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n",
        "\n",
        "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
        "        # rnn_input: (1, N, hidden_size*2 + embedding_size)\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "        # outputs shape: (1, N, hidden_size)\n",
        "\n",
        "        predictions = self.fc(outputs).squeeze(0)\n",
        "        # predictions: (N, hidden_size)\n",
        "\n",
        "        return predictions, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Seq2SeqAttn(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2SeqAttn, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = len(hindi_alpha2index)\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device_gpu)\n",
        "        encoder_states, hidden, cell = self.encoder(source)\n",
        "\n",
        "        # First input will be <SOS> token\n",
        "        x = target[0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # At every time step use encoder_states and update hidden, cell\n",
        "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
        "\n",
        "            # Store prediction for current time step\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            # With probability of teacher_force_ratio we take the actual next word\n",
        "            # otherwise we take the word that the Decoder predicted it to be.\n",
        "            # Teacher Forcing is used so that the model gets used to seeing\n",
        "            # similar inputs at training and testing time, if teacher forcing is 1\n",
        "            # then inputs at test time might be completely different than what the\n",
        "            # network is used to. This was a long comment.\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class Seq2SeqAttn(nn.Module):\n",
        "#     def __init__(self, encoder, decoder):\n",
        "#         super(Seq2SeqAttn,self).__init__()\n",
        "#         self.encoder = encoder\n",
        "#         self.decoder = decoder\n",
        "\n",
        "#     def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "#         batch_size = source.shape[1]\n",
        "#         target_len = target.shape[0]\n",
        "\n",
        "#         target_vocab_size = len(hindi_alpha2index)\n",
        "\n",
        "#         outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device_gpu)\n",
        "        \n",
        "#         encoder_states, hidden, cell = self.encoder(source)\n",
        "\n",
        "#         x = target[0]\n",
        "\n",
        "#         for t in range(1, target_len):\n",
        "#             output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
        "\n",
        "#             outputs[t] = output\n",
        "#             # ouptut is N, hindi_vocab_size\n",
        "#             best_guess = output.argmax(1)\n",
        "\n",
        "#             x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "        \n",
        "#         return outputs\n",
        "\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 11859 Loss 0.8920754194259644\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb Cell 36\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m loss_arr\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m display_freq\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch\u001b[39m%\u001b[39mdisplay_freq \u001b[39m==\u001b[39m display_freq\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
            "File \u001b[0;32m~/Documents/ML/my-venv1/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/ML/my-venv1/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#training hyperparameters\n",
        "\n",
        "num_epochs = 17000\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "\n",
        "load_model = False\n",
        "\n",
        "\n",
        "input_size_encoder = len(eng_alpha2index)\n",
        "input_size_decoder = len(hindi_alpha2index)\n",
        "output_size = len(hindi_alpha2index)\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "\n",
        "hidden_size = 1024\n",
        "num_layers = 1 # 2 for without attention.\n",
        "encoder_dropout = 0.5\n",
        "decoder_dropout = 0.5\n",
        "\n",
        "#Tensorboard\n",
        "\n",
        "# writer = SummaryWriter(f'runs/Loss_plot')\n",
        "step = 0\n",
        "# train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "#     (train_data, test_data),\n",
        "#     batch_size = batch_size,\n",
        "#     sort_within_batch = True,\n",
        "#     sort_key = lambda x: len(x.src),\n",
        "#     device=device_gpu\n",
        "# )\n",
        "\n",
        "encoderAttn_net = EncoderAttn(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, encoder_dropout).to(device_gpu)\n",
        "decoderAttn_net = DecoderAttn(input_size_decoder, decoder_embedding_size, hidden_size,output_size, num_layers, decoder_dropout).to(device_gpu)\n",
        "\n",
        "model = Seq2SeqAttn(encoderAttn_net, decoderAttn_net).to(device_gpu)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "pad_idx = eng_alpha2index['-PAD-']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "loss_arr = []\n",
        "for epoch in range(num_epochs):\n",
        "    eng_batch, hindi_batch = train_data.get_batch(batch_size)\n",
        "    input = input_rep(eng_batch[i], eng_alpha2index, device_gpu)\n",
        "    gt = gt_rep(hindi_batch[i], hindi_alpha2index, device_gpu)\n",
        "    inp_data = input.to(torch.int64)\n",
        "    target = gt\n",
        "    output = model(inp_data, target)\n",
        "    #output = tar_len, batch_size, output_size\n",
        "\n",
        "    #(N, 10) and terget would be (N)\n",
        "\n",
        "    output = output.reshape(-1, output.shape[2])\n",
        "    target = target.reshape(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(output, target)\n",
        "    loss_arr.append(loss.item())\n",
        "    loss.backward()\n",
        "    display_freq=5\n",
        "    if epoch%display_freq == display_freq-1:\n",
        "            clear_output(wait=True)\n",
        "            \n",
        "            print('Iteration', epoch, 'Loss', loss.item())\n",
        "            # print('Top-1:', eval(net, len(X_test), 1, X_test, y_test), 'Top-2:', eval(net, len(X_test), 2, X_test, y_test))\n",
        "            plt.figure()\n",
        "            plt.plot(loss_arr[1:epoch], '-*')\n",
        "            plt.xlabel('Iteration')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.show()\n",
        "            print('\\n\\n')\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "    optimizer.step()\n",
        "\n",
        "    step +=1\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_sentence(model, english, hindi, device, max_length=50):\n",
        "    # print(sentence)\n",
        "\n",
        "    input = input_rep(english, eng_alpha2index, device_gpu)\n",
        "    gt = gt_rep(hindi, hindi_alpha2index, device_gpu)\n",
        "\n",
        "    # # Convert to Tensor\n",
        "    # sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        encoder_states, hidden, cell = model.encoder(input)\n",
        "\n",
        "    outputs = [hindi_alpha2index[\"-PAD-\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(previous_word, encoder_states, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == hindi_alpha2index[\"-EOS-\"]:\n",
        "            break\n",
        "    print(outputs)\n",
        "    translated_sentence = [hindi_index2alpha[idx] for idx in outputs]\n",
        "    hindi_output = ''\n",
        "    for char in outputs:\n",
        "        hindi_output += hindi_index2alpha[char]\n",
        "\n",
        "    # remove start token\n",
        "    return translated_sentence, hindi_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "english = 'LOVE'\n",
        "hindi = 'इडेलिया'\n",
        "device = device_gpu\n",
        "\n",
        "translate_sentence(model, english, hindi, device, max_length=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 45, 73, 49, 63, 48, 63, 130]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(['-PAD-', 'ब', 'ै', 'र', 'ा', 'य', 'ा', '-EOS-'], '-PAD-बैराया-EOS-')"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "english = 'BAIRATHI'\n",
        "hindi = 'इडेलिया'\n",
        "device = device_gpu\n",
        "\n",
        "translate_sentence(model, english, hindi, device, max_length=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6w8ffT3w4lkK"
      },
      "outputs": [],
      "source": [
        "MAX_OUTPUT_CHARS = 30\n",
        "class Transliteration_EncoderDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Transliteration_EncoderDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.encoder_rnn_cell = nn.GRU(input_size, hidden_size)\n",
        "        self.decoder_rnn_cell = nn.GRU(output_size, hidden_size)\n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "        \n",
        "    def forward(self, input, max_output_chars = MAX_OUTPUT_CHARS, device = 'cpu', ground_truth = None):\n",
        "        out, hidden = self.encoder_rnn_cell(input)\n",
        "        decoder_state = hidden\n",
        "        decoder_input = torch.zeros(1, 1, self.output_size).to(device)\n",
        "        outputs = []\n",
        "        for i in range(max_output_chars):\n",
        "            out, decoder_state = self.decoder_rnn_cell(decoder_input, decoder_state)\n",
        "            out = self.h2o(decoder_state)\n",
        "            out = self.softmax(out)\n",
        "            outputs.append(out.view(1, -1))\n",
        "            # Get OHE from Softmax: https://discuss.pytorch.org/t/softmax-to-one-hot/37302\n",
        "            max_idx = torch.argmax(out, 2, keepdim=True)\n",
        "            if not ground_truth is None:\n",
        "                max_idx = ground_truth[i].reshape(1, 1, 1)\n",
        "            one_hot = torch.FloatTensor(out.shape).to(device)\n",
        "            one_hot.zero_()\n",
        "            one_hot.scatter_(2, max_idx, 1)\n",
        "            decoder_input = one_hot.detach()\n",
        "            \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XzM74zGtVbGB"
      },
      "source": [
        "### Encoder-Decoder with Attention (Type 1)\n",
        "(mechanism based on [PyTorch Docs](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_NRMybNU7rzZ"
      },
      "outputs": [],
      "source": [
        "MAX_ENCODER_STEPS = 30\n",
        "\n",
        "class Transliteration_EncoderDecoderAttention(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, max_encoder_steps = MAX_ENCODER_STEPS):\n",
        "        super(Transliteration_EncoderDecoderAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.encoder_rnn_cell = nn.GRU(input_size, hidden_size)\n",
        "        self.decoder_rnn_cell = nn.GRU(hidden_size, hidden_size)\n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "        \n",
        "        self.max_encoder_steps = max_encoder_steps\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, max_encoder_steps)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.out2hidden = nn.Linear(self.output_size, self.hidden_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, input, max_output_chars = MAX_OUTPUT_CHARS, device = 'cpu', ground_truth = None):\n",
        "        out, hidden = self.encoder_rnn_cell(input)\n",
        "        decoder_state = hidden\n",
        "        \n",
        "        encoder_outputs = torch.zeros(self.max_encoder_steps, self.hidden_size, device=device)\n",
        "        for i in range(out.shape[0]):\n",
        "            encoder_outputs[i] = out[i, 0]\n",
        "        \n",
        "        decoder_input = torch.zeros(1, 1, self.output_size).to(device)\n",
        "        outputs = []\n",
        "        for i in range(max_output_chars):\n",
        "            embedding = self.out2hidden(decoder_input)\n",
        "            attn_weights = F.softmax(\n",
        "                self.attn(torch.cat((embedding[0], decoder_state[0]), 1)), dim=1)\n",
        "            attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "            decoder_input = torch.cat((embedding[0], attn_applied[0]), 1)\n",
        "            decoder_input = self.attn_combine(decoder_input).unsqueeze(0)\n",
        "            decoder_input = F.relu(decoder_input)\n",
        "            \n",
        "            \n",
        "            out, decoder_state = self.decoder_rnn_cell(decoder_input, decoder_state)\n",
        "            out = self.h2o(decoder_state)\n",
        "            out = self.softmax(out)\n",
        "            outputs.append(out.view(1, -1))\n",
        "            # Get OHE from Softmax: https://discuss.pytorch.org/t/softmax-to-one-hot/37302\n",
        "            max_idx = torch.argmax(out, 2, keepdim=True)\n",
        "            if not ground_truth is None:\n",
        "                max_idx = ground_truth[i].reshape(1, 1, 1)\n",
        "            one_hot = torch.zeros(out.shape, device = device)\n",
        "            one_hot.scatter_(2, max_idx, 1)\n",
        "            decoder_input = one_hot.detach()\n",
        "            \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NEg49N9e7oTY"
      },
      "source": [
        "### Encoder-Decoder with Attention (Type 2)\n",
        "(mechanism based on [Mitesh's lectures on Attention](https://www.cse.iitm.ac.in/~miteshk/CS7015_2018.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8z-1QDAz8F_d"
      },
      "outputs": [],
      "source": [
        "class Transliteration_EncoderDecoderAttention_Type2(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Transliteration_EncoderDecoderAttention_Type2, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.encoder_rnn_cell = nn.GRU(input_size, hidden_size)\n",
        "        self.decoder_rnn_cell = nn.GRU(hidden_size*2, hidden_size)\n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "        \n",
        "        self.U = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.W = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size, 1)\n",
        "        self.out2hidden = nn.Linear(self.output_size, self.hidden_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, input, max_output_chars = MAX_OUTPUT_CHARS, device = 'cpu', ground_truth = None):\n",
        "        encoder_outputs, hidden = self.encoder_rnn_cell(input)\n",
        "        encoder_outputs = encoder_outputs.view(-1, self.hidden_size)\n",
        "        decoder_state = hidden\n",
        "        \n",
        "        decoder_input = torch.zeros(1, 1, self.output_size).to(device)\n",
        "        outputs = []\n",
        "        U = self.U(encoder_outputs)\n",
        "        \n",
        "        for i in range(max_output_chars):\n",
        "            W = self.W(decoder_state.view(1, -1).repeat(encoder_outputs.shape[0], 1))\n",
        "            V = self.attn(torch.tanh(U + W))\n",
        "            attn_weights = F.softmax(V.view(1, -1), dim = 1)\n",
        "            \n",
        "            attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "            embedding = self.out2hidden(decoder_input)\n",
        "            decoder_input = torch.cat((embedding[0], attn_applied[0]), 1).unsqueeze(0)\n",
        "            \n",
        "            out, decoder_state = self.decoder_rnn_cell(decoder_input, decoder_state)\n",
        "            out = self.h2o(decoder_state)\n",
        "            out = self.softmax(out)\n",
        "            outputs.append(out.view(1, -1))\n",
        "            # Get OHE from Softmax: https://discuss.pytorch.org/t/softmax-to-one-hot/37302\n",
        "            max_idx = torch.argmax(out, 2, keepdim=True)\n",
        "            if not ground_truth is None:\n",
        "                max_idx = ground_truth[i].reshape(1, 1, 1)\n",
        "            one_hot = torch.zeros(out.shape, device=device)\n",
        "            one_hot.scatter_(2, max_idx, 1) # In dim 2, set max_idx's as 1\n",
        "            decoder_input = one_hot.detach()\n",
        "            \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cyE2tSnmAW6x"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H893cimDtTUE"
      },
      "source": [
        "### Core Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "m804jsH7AXSV"
      },
      "outputs": [],
      "source": [
        "def train_batch(net, opt, criterion, batch_size, device = 'cpu', teacher_force = False):\n",
        "    net.train().to(device)\n",
        "    opt.zero_grad()\n",
        "    eng_batch, hindi_batch = train_data.get_batch(batch_size)\n",
        "    \n",
        "    total_loss = 0\n",
        "    for i in range(batch_size):\n",
        "        input = word_rep(eng_batch[i], eng_alpha2index, device)\n",
        "        gt = gt_rep(hindi_batch[i], hindi_alpha2index, device)\n",
        "        outputs = net(input, gt.shape[0], device, ground_truth = gt if teacher_force else None)\n",
        "        \n",
        "        for index, output in enumerate(outputs):\n",
        "            loss = criterion(output, gt[index]) / batch_size\n",
        "            loss.backward(retain_graph = True)\n",
        "            total_loss += loss\n",
        "        \n",
        "    opt.step()\n",
        "    return total_loss/batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p-eZaBxstWz9"
      },
      "source": [
        "### Training Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Rjto129ssrpr"
      },
      "outputs": [],
      "source": [
        "def train_setup(net, lr = 0.01, n_batches = 100, batch_size = 10, momentum = 0.9, display_freq=5, device = 'cpu'):\n",
        "    net = net.to(device)\n",
        "    criterion = nn.NLLLoss(ignore_index = -1)\n",
        "    opt = optim.Adam(net.parameters(), lr=lr)\n",
        "    teacher_force_upto = n_batches//3\n",
        "    \n",
        "    loss_arr = np.zeros(n_batches + 1)\n",
        "    \n",
        "    for i in range(n_batches):\n",
        "        loss_arr[i+1] = (loss_arr[i]*i + train_batch(net, opt, criterion, batch_size, device = device, teacher_force = i<teacher_force_upto ))/(i + 1)\n",
        "        \n",
        "        if i%display_freq == display_freq-1:\n",
        "            clear_output(wait=True)\n",
        "            \n",
        "            print('Iteration', i, 'Loss', loss_arr[i])\n",
        "            # print('Top-1:', eval(net, len(X_test), 1, X_test, y_test), 'Top-2:', eval(net, len(X_test), 2, X_test, y_test))\n",
        "            plt.figure()\n",
        "            plt.plot(loss_arr[1:i], '-*')\n",
        "            plt.xlabel('Iteration')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.show()\n",
        "            print('\\n\\n')\n",
        "            \n",
        "    #print('Top-1:', eval(net, len(X_test), 1, X_test, y_test, device), 'Top-2:', eval(net, len(X_test), 2, X_test, y_test, device))\n",
        "    torch.save(net, 'model.pt')\n",
        "    return loss_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TZY6RvqLtdX8"
      },
      "source": [
        "### Training without Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1oQ3ZIWvtjfN"
      },
      "outputs": [],
      "source": [
        "net = Transliteration_EncoderDecoder(len(eng_alpha2index), 256, len(hindi_alpha2index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "colab_type": "code",
        "id": "E6LjVKQfoVMU",
        "outputId": "f79e9eae-393e-48d8-b117-075fdc7b185a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1999 Loss 0.1844225972890854\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0.        , 0.48061725, 0.49358338, ..., 0.18447448, 0.1844226 ,\n",
              "       0.18437725])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_setup(net, lr=0.001, n_batches=2000, batch_size = 64, display_freq=10, device = device_gpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZWQss1PvmmJw"
      },
      "source": [
        "### Training with Attention Type 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PJOGewYJY_yR"
      },
      "outputs": [],
      "source": [
        "net2 = Transliteration_EncoderDecoderAttention(len(eng_alpha2index), 256, len(hindi_alpha2index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "4AKEEGp1ZOKK",
        "outputId": "15639d3a-9cc4-457b-a0ce-2602a5bfa050"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 2049 Loss 0.1293310672044754\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABIQ0lEQVR4nO3de1hVZcL38R8HARU5JMpJEk3TTAWPjKVNPZLg65RONUNOSTFNPpn1vA5a6Vup1TSoHS6mdLTxyfFQU9ZMWVMNZSR2IjU8po6TB1QUUFCOKuDe6/3DYU87QYF9WMD+fq5rXxNr3eve93JN8us+rNvLMAxDAAAAHsTb7AYAAAC4GwEIAAB4HAIQAADwOAQgAADgcQhAAADA4xCAAACAxyEAAQAAj+NrdgNaI6vVquPHj6tLly7y8vIyuzkAAKAJDMNQZWWloqKi5O196T4eAlADjh8/rpiYGLObAQAAWuDo0aPq0aPHJcsQgBrQpUsXSRf+AIOCgkxuDQAAaIqKigrFxMTYfo9fCgGoAfXDXkFBQQQgAADamKZMX2ESNAAA8DgEIAAA4HEIQAAAwOMQgAAAgMchAAEAAI9DAAIAAB6HAAQAADwOAQgAAHgcAhAAAPA4BCAAAOBxCEButrOgTJP/9I12FpSZ3RQAADwWAcjN3tl6TLkHS/XO1mNmNwUAAI/FZqhuUHD6jE5X1+lM7Xm9t/1C8Pn7juO6Y1gPGYYU2rmDeoR2MrmVAAB4DgKQG4xeuOGiY6eqa/Wzl7+0/Zy/YII7mwQAgEdjCMwNMlPi5evtZXfM+Pf/+np7KTMl3u1tAgDAk9ED5AaThkSrT/dAux6feuumX6+B0cEmtAoAAM9FD5BJvLwuXwYAALgGPUBu0jXQT4H+PqqqsWhgVJC8vb1UWHZOXQP9zG4aAAAehwDkJpHBHfVIUj/Ne3+PokM7atndw1Rrscrf18fspgEA4HEYAnOjgA4Xwo7FasjLy4vwAwCASQhAbuTjfeGP+7zVuExJAADgSgQgN6pfCr/tSBlbYQAAYCICkBv5/DsAlZ+tYysMAABM1CoC0JIlSxQbG6uAgAAlJCRo8+bNjZZduXKlvLy87D4BAQF2ZQzD0Ny5cxUZGamOHTsqMTFR33//vatvo1EFp89oV0G5jpedtR37+47j+u5YuXYVlKvg9BnT2gYAgCcyfRXY2rVrlZ6ermXLlikhIUGZmZlKSkrSvn371L179wavCQoK0r59+2w/e/3opTqLFi3SSy+9pFWrVqlXr1568sknlZSUpD179lwUltyhoa0wStkKAwAA05jeA/Tiiy/q/vvvV1pamgYMGKBly5apU6dOWrFiRaPXeHl5KSIiwvYJDw+3nTMMQ5mZmXriiSc0ceJEDR48WKtXr9bx48e1bt06N9zRxRraCqMeW2EAAOB+pgag2tpa5eXlKTEx0XbM29tbiYmJys3NbfS6qqoq9ezZUzExMZo4caJ2795tO3fo0CEVFRXZ1RkcHKyEhIRG66ypqVFFRYXdx5kmDYnWuunXN3hu3fTrNWlItFO/DwAAXJqpAaikpEQWi8WuB0eSwsPDVVRU1OA1/fr104oVK/Tee+/ptddek9Vq1XXXXaeCggJJsl3XnDozMjIUHBxs+8TExDh6a3YKTp/R98VVDZ77vriKOUAAALiZ6XOAmmvUqFEaNWqU7efrrrtO11xzjV555RU988wzLapzzpw5Sk9Pt/1cUVHh1BDU0Byger99a7sk5gABAOBOpvYAhYWFycfHR8XFxXbHi4uLFRER0aQ6OnTooCFDhmj//v2SZLuuOXX6+/srKCjI7gMAANovUwOQn5+fhg0bpuzsbNsxq9Wq7Oxsu16eS7FYLNq1a5ciIyMlSb169VJERIRdnRUVFdq0aVOT63S2zJR4+TSy/buPF5OgAQBwN9NXgaWnp2v58uVatWqV9u7dq2nTpqm6ulppaWmSpNTUVM2ZM8dW/umnn9Ynn3yigwcPauvWrbr77rt1+PBh/eY3v5F0YYXYjBkz9Lvf/U7vv/++du3apdTUVEVFRWnSpElm3KKGx4ZqRmLfBs/NSOyr4bGhbm4RAACezfQ5QCkpKTp58qTmzp2roqIixcfHKysryzaJ+ciRI/L2/k9OO336tO6//34VFRUpNDRUw4YN09dff60BAwbYyjz66KOqrq7W1KlTVVZWptGjRysrK8uUdwBJl54D9ML6f+mF9f/Sl4/dpB6hndzYKgAAPJeXYRjszPkjFRUVCg4OVnl5uVPmA63bdkwz1m6/bDkmQgMA0HLN+f1t+hCYJ5g0JFozx13d6HlehggAgHsRgNzkpn4Nb+sh8TJEAADcjQDkJl0D/RTSsYPZzQAAACIAuY3FamjRHYMV6P+feec+3l4K6dhBJyvP8TZoAADcyPRVYJ6ioZVgVquhsrN1Slv5rSQmQQMA4C70ALlJQzvC1y+/YxI0AADuRQ+Qm0waEq0+3QP1s5e/vOjcuunXa2B0sAmtAgDAM9EDBAAAPA4ByE0KTp/RycpzdivBmAQNAIA5GAJzk4YmQVuYBA0AgCnoAXKThiZB12MSNAAA7kUPkJswCRoAgNaDHiAAAOBxCEBu1DXQT90C/eXz76GwoABfdQv0V9dAP5NbBgCAZyEAuUnB6TMqqazVK6lDVT8TyDCkV1KHqqSyllVgAAC4EXOA3KShVWCVNed12x9zbT+zCgwAAPegB8hNWAUGAEDrQQ+Qm7AKDACA1oMeIAAA4HHoAXKj+lVgFsPQqepahXfxl9UQq8AAAHAzeoDcKDK4o76cfZNujYuUdGEvsKVThioyuKPJLQMAwLMQgNzM39dHHf0udLwdLz+nD3YUmtwiAAA8D0NgblRw+oxOV9ep/Eyt7djfdxzXHcN6yDCk0M4d1CO0k4ktBADAMxCA3KihdwGdqq61WxnGu4AAAHA9hsDcqKF3ARn//l/eBQQAgPvQA+RGvAsIAIDWgR4gk3k1/HJoAADgQvQAuVnXQD8FBfiq4tx59ezaScEdO6iw7BzvAgIAwI0IQG4WGdxRz/1isP57zVYFd+yg96Zfr1qLVf6+PmY3DQAAj8EQmAm6+HeQJH1fXKVdx8oJPwAAuBkByAT+HS78sZ+ts+idrcdMbg0AAJ6HITA3qn8R4tFTZ23HeBEiAADuRwByI16ECABA69AqhsCWLFmi2NhYBQQEKCEhQZs3b27SdW+++aa8vLw0adIku+P33nuvvLy87D7JyckuaHnz8CJEAABaB9MD0Nq1a5Wenq558+Zp69atiouLU1JSkk6cOHHJ6/Lz8zVr1iyNGTOmwfPJyckqLCy0fd544w1XNL9ZJg2J1rrp1zd4bt306zVpSLSbWwQAgGcyPQC9+OKLuv/++5WWlqYBAwZo2bJl6tSpk1asWNHoNRaLRXfddZeeeuop9e7du8Ey/v7+ioiIsH1CQ0Mbra+mpkYVFRV2HwAA0H6ZGoBqa2uVl5enxMRE2zFvb28lJiYqNze30euefvppde/eXffdd1+jZXJyctS9e3f169dP06ZNU2lpaaNlMzIyFBwcbPvExMS07IaaoGugn8J+8NLDKzr7qVugPy9CBADAjUwNQCUlJbJYLAoPD7c7Hh4erqKiogav+fLLL/Xqq69q+fLljdabnJys1atXKzs7WwsXLtTGjRs1fvx4WSyWBsvPmTNH5eXlts/Ro0dbflOXUHD6jEoqa/XKlGG2Y4Zh6JXUoSqprFXB6TMu+V4AAGCvTa0Cq6ys1JQpU7R8+XKFhYU1Wu7OO++0/fOgQYM0ePBgXXXVVcrJydHYsWMvKu/v7y9/f3+XtPmHGloFdvpMnW774396u1gFBgCA65naAxQWFiYfHx8VFxfbHS8uLlZERMRF5Q8cOKD8/Hzdcsst8vX1la+vr1avXq33339fvr6+OnDgQIPf07t3b4WFhWn//v0uuY+mamgVWD1WgQEA4D6mBiA/Pz8NGzZM2dnZtmNWq1XZ2dkaNWrUReX79++vXbt2afv27bbPrbfeqptuuknbt29vdO5OQUGBSktLFRkZ6bJ7aQpWgQEA0DqYPgSWnp6ue+65R8OHD9fIkSOVmZmp6upqpaWlSZJSU1MVHR2tjIwMBQQEaODAgXbXh4SESJLteFVVlZ566indfvvtioiI0IEDB/Too4+qT58+SkpKcuu9AQCA1sn0AJSSkqKTJ09q7ty5KioqUnx8vLKysmwTo48cOSJv76Z3VPn4+Gjnzp1atWqVysrKFBUVpXHjxumZZ55xyzyfy+kaeGHV16nqGlkMKSjAV/6+PqwCAwDAjbwMwzAuX8yzVFRUKDg4WOXl5QoKCnJavfV7gdVZLfrFsm9ksRrqEuCrVb8eoQ7ePuwFBgCAA5rz+9v0HiBP0tAqsMpz51kFBgCAm5n+JmhPwiowAABaB3qA3GjSkGj16R5ot/t7vXXTr9fA6GATWgUAgOehBwgAAHgcApCb1a8C8/e98Effyc+HvcAAAHAzApAb2fYCSx1qO2YYYi8wAADcjDlAbtTQKrCzdRZWgQEA4Gb0ALkRq8AAAGgd6AFyI1aBAQDQOtADBAAAPA4ByM3qV4FFhQTYjnXswF5gAAC4EwHIzSxWQ8umDNUdQ3rYjnXy81FpVa12FZSzEgwAADdgDpCbNbQS7FR1rd28IFaCAQDgWvQAuVlDK8GMf/8vK8EAAHAPeoDcjJVgAACYjx4gAADgcQhAblZw+oxOVp5TYMB/Ot98vL0U0rGDTlaeYxI0AABuwBCYmzU0CdpiNVR2tk5pK7+VxCRoAABcjR4gN2M7DAAAzEcAcrPhsaF6/hdxDZ57/hdxGh4b6uYWAQDgeRgCc7OGhsDqzVi7XRJDYAAAuBo9QG6WmRIvn0aGwHwYAgMAwC0IQG42aUi03pt+fYPn3pt+vSYNiXZziwAA8DwEIAAA4HEIQG5W/x6gkI4d9MOBsEB/H94DBACAmzAJ2s0amwRdVWPhPUAAALgJPUBulpkSL5+G50DLx0uaefPV9AIBAOBiBCA3mzQkWhaj4XMWQ3ph/b8uuVQeAAA4jgBkgpnjrm70HG+DBgDA9QhAJri+T1cF+jc8/Yq3QQMA4HpMgjbBbX/MbfQcb4MGAMD16AEyAW+DBgDAXK0iAC1ZskSxsbEKCAhQQkKCNm/e3KTr3nzzTXl5eWnSpEl2xw3D0Ny5cxUZGamOHTsqMTFR33//vQta3jLDY0P1QiMbor7AEBgAAC5negBau3at0tPTNW/ePG3dulVxcXFKSkrSiRMnLnldfn6+Zs2apTFjxlx0btGiRXrppZe0bNkybdq0SZ07d1ZSUpLOnTvnqttoltELN9iGun5sxtrtrAIDAMDFTA9AL774ou6//36lpaVpwIABWrZsmTp16qQVK1Y0eo3FYtFdd92lp556Sr1797Y7ZxiGMjMz9cQTT2jixIkaPHiwVq9erePHj2vdunUuvpummfuzAWpkBEzeXhfOAwAA1zE1ANXW1iovL0+JiYm2Y97e3kpMTFRubuMThZ9++ml1795d991330XnDh06pKKiIrs6g4ODlZCQ0GidNTU1qqiosPu40tMf7JG1kXcBWY0L5wEAgOuYGoBKSkpksVgUHh5udzw8PFxFRUUNXvPll1/q1Vdf1fLlyxs8X39dc+rMyMhQcHCw7RMTE9PcW2mWC2+DbmQStBeToAEAcDXTh8Cao7KyUlOmTNHy5csVFhbmtHrnzJmj8vJy2+fo0aNOq7shk4ZE69V7hzd47tV7h2vSkGiXfj8AAJ7O1PcAhYWFycfHR8XFxXbHi4uLFRERcVH5AwcOKD8/X7fccovtmNVqlST5+vpq3759tuuKi4sVGRlpV2d8fHyD7fD395e/v7+jt9NkBafPqOxMXYPnys7UqeD0GfUI7eS29gAA4GlM7QHy8/PTsGHDlJ2dbTtmtVqVnZ2tUaNGXVS+f//+2rVrl7Zv32773Hrrrbrpppu0fft2xcTEqFevXoqIiLCrs6KiQps2bWqwTjOwCgwAAHOZ/ibo9PR03XPPPRo+fLhGjhypzMxMVVdXKy0tTZKUmpqq6OhoZWRkKCAgQAMHDrS7PiQkRJLsjs+YMUO/+93v1LdvX/Xq1UtPPvmkoqKiLnpfkFkyU+I16+0dOt/ATGhfby8938g7ggAAgHOYHoBSUlJ08uRJzZ07V0VFRYqPj1dWVpZtEvORI0fk7d28jqpHH31U1dXVmjp1qsrKyjR69GhlZWUpICDAFbfQbJOGRCsowFe/XvXtReee/0WcruoWyDAYAAAu5GUYRiMLsj1XRUWFgoODVV5erqCgIJd8R+zsDy9bhv3AAABouub8/m5Tq8Dak/m3NP6yQ1/2AwMAwKUIQCZJHBCu5+4Y1OC559kPDAAAlzJ9DpCnutRKr/oVYgyBAQDgGvQAmYT9wAAAMA8ByCTsBwYAgHkIQCaZ+7MBjf7he4seIAAAXIkAZJKnP9gjayPnrP8+X3D6jDubBACAxyAAmaQpy9zZEgMAANcgAJmEHd8BADAPAchE94/p1eg55gEBAOA6BCATLf/iUKPn6ucBAQAA5yMAtXJMhAYAwPkIQK0cE6EBAHA+ApCJLrcSzEvMAwIAwBUIQCa63EowQ8wDAgDAFQhAAADA4xCA2gAmQgMA4FwEIJPxRmgAANyPAGQy3ggNAID7EYAAAIDHIQABAACPQwBqI5gIDQCA8xCAWgEmQgMA4F4EoFaAidAAALgXAQgAAHgcAlAbwjwgAACcgwDUSjAPCAAA9yEAtRJNnQf0bf4pF7cEAID2jwDUxtyxLNfsJgAA0OYRgFqR+bcMaFI55gIBAOAYAlArcu/1vZpUjrlAAAA4hgDUytyVEGN2EwAAaPcIQK3MQ//Vt0nlmAwNAEDLtYoAtGTJEsXGxiogIEAJCQnavHlzo2XfeecdDR8+XCEhIercubPi4+O1Zs0auzL33nuvvLy87D7Jycmuvg2niAzuqDtH9LhsOSZDAwDQcqYHoLVr1yo9PV3z5s3T1q1bFRcXp6SkJJ04caLB8ldccYUef/xx5ebmaufOnUpLS1NaWpo+/vhju3LJyckqLCy0fd544w133I5T/N/Eq5tU7h+7Cl3cEgAA2icvwzAMMxuQkJCgESNGaPHixZIkq9WqmJgYPfzww5o9e3aT6hg6dKgmTJigZ555RtKFHqCysjKtW7euSdfX1NSopqbG9nNFRYViYmJUXl6uoKCg5t2QkyS+kKP9J6svWy5/wQQ3tAYAgNavoqJCwcHBTfr9bWoPUG1trfLy8pSYmGg75u3trcTEROXmXn6IxzAMZWdna9++fbrhhhvszuXk5Kh79+7q16+fpk2bptLS0kbrycjIUHBwsO0TE2P+ROQ1v0loUrnXcvNd2xAAANohUwNQSUmJLBaLwsPD7Y6Hh4erqKio0evKy8sVGBgoPz8/TZgwQS+//LJuvvlm2/nk5GStXr1a2dnZWrhwoTZu3Kjx48fLYrE0WN+cOXNUXl5u+xw9etQ5N+iAyOCO+knvKy5b7on3druhNQAAtC++ZjegJbp06aLt27erqqpK2dnZSk9PV+/evXXjjTdKku68805b2UGDBmnw4MG66qqrlJOTo7Fjx15Un7+/v/z9/d3V/Ca7f0wvfXPw8qu9XsvN192jYl3fIAAA2glTe4DCwsLk4+Oj4uJiu+PFxcWKiIho9Dpvb2/16dNH8fHxmjlzpu644w5lZGQ0Wr53794KCwvT/v37ndZ2dxh7TYT+z8Dwy5Z74r3dLIsHAKAZTA1Afn5+GjZsmLKzs23HrFarsrOzNWrUqCbXY7Va7SYx/1hBQYFKS0sVGRnpUHvN8OQt1zapHMviAQBoOtOXwaenp2v58uVatWqV9u7dq2nTpqm6ulppaWmSpNTUVM2ZM8dWPiMjQ+vXr9fBgwe1d+9evfDCC1qzZo3uvvtuSVJVVZUeeeQRffPNN8rPz1d2drYmTpyoPn36KCkpyZR7dERkcEc9dFPvJpWlFwgAgKYxfQ5QSkqKTp48qblz56qoqEjx8fHKysqyTYw+cuSIvL3/k9Oqq6v14IMPqqCgQB07dlT//v312muvKSUlRZLk4+OjnTt3atWqVSorK1NUVJTGjRunZ555plXO82mKu34Sq8UbDl623B3LcvXXB0ZpeOzlJ08DAODJTH8PUGvUnPcIuMsbm/I1592mrfji3UAAAE/UZt4DhKabnBCrXl07Naksb4gGAODSCEBtyBM/u6ZJ5aa9vlUFp8+4uDUAALRdBKA2ZOw1EXpgTK8mlR29cAMhCACARhCA2pjxg5u+lH/0wg0ubAkAAG0XAaiNiYsJ1cuT45tcnqXxAABcjADUBt0SF627E5q2Yesdy3KZFA0AwI+0KAAdPXpUBQUFtp83b96sGTNm6E9/+pPTGoZL+8Xwpu9YP+31rYQgAAB+oEUB6Fe/+pU2bLgwv6SoqEg333yzNm/erMcff1xPP/20UxuIhjV3KGza61sZDgMA4N9aFIC+++47jRw5UpL01ltvaeDAgfr666/1+uuva+XKlc5sHy7hlrhozby5b5PLs18YAAAXtCgA1dXV2baV+PTTT3XrrbdKkvr376/CQoZa3OmO4TEK8G36Y2QoDACAFgaga6+9VsuWLdMXX3yh9evXKzk5WZJ0/Phxde3a1akNxKVFBnfUmt+MbHJ55gMBANDCALRw4UK98soruvHGGzV58mTFxcVJkt5//33b0BjcZ0RsV/3lNwlNLs+bogEAnq7Fm6FaLBZVVFQoNDTUdiw/P1+dOnVS9+7dndZAM7TGzVCb4vviSiVnfi5LE5/o7yZeq7tHxbq0TQAAuIvLN0M9e/asampqbOHn8OHDyszM1L59+9p8+GnL+oZ30ZK7hjS5/BPv7WY4DADgkVoUgCZOnKjVq1dLksrKypSQkKAXXnhBkyZN0tKlS53aQDRP8sCoZq0MY3k8AMATtSgAbd26VWPGjJEk/fWvf1V4eLgOHz6s1atX66WXXnJqA9F8dwyPkb+vV9PLL8slBAEAPEqLAtCZM2fUpUsXSdInn3yi2267Td7e3vrJT36iw4cPO7WBaL7I4I56rRmToiVCEADAs7QoAPXp00fr1q3T0aNH9fHHH2vcuHGSpBMnTrSpScPt2YjYrs16U7TEvmEAAM/RogA0d+5czZo1S7GxsRo5cqRGjRol6UJv0JAhTZ+EC9e6JS5aH/3PaHVsxnDYtNe3auFHe13YKgAAzNfiZfBFRUUqLCxUXFycvL0v5KjNmzcrKChI/fv3d2oj3a2tLoNvzOHSat384kbVNnV9vKTkAeFaljrcha0CAMC5mvP7u8UBqF79rvA9evRwpJpWpb0FIEnK+u64HnhtW7OuGd4zRJl3DlGP0E4uahUAAM7j8vcAWa1WPf300woODlbPnj3Vs2dPhYSE6JlnnpHVam1Ro+FayQOjlPnLuGZd8+3hMo1euIHJ0QCAdqdFAejxxx/X4sWLtWDBAm3btk3btm3T73//e7388st68sknnd1GOMmkoT20Om1Es6+7Y1muluXsd0GLAAAwR4uGwKKiorRs2TLbLvD13nvvPT344IM6duyY0xpohvY4BPZDX+8v0a/+d1Ozr7smIlD/mPFTF7QIAADHuXwI7NSpUw1OdO7fv79OnWK4pLW7rk+Yds0fp+k39mrWdXuLqhQ7+0M9/f53LmoZAADu0aIAFBcXp8WLF190fPHixRo8eLDDjYLrdQnooEeSB2h28tXNvnbF14cVO/tDZa7f54KWAQDgei0aAtu4caMmTJigK6+80vYOoNzcXB09elQfffSRbZuMtqq9D4H92PKNB/TsP/7Zomu9vaTn7his24fFOLlVAAA0j8uHwH7605/qX//6l37+85+rrKxMZWVluu2227R7926tWbOmRY2Gee7/6VV6NXVYi661GtLMt3fqruXfOLlVAAC4jsPvAfqhHTt2aOjQobJYLM6q0hSe1gNUr6WTo+t5SZp+01WaldS2X4QJAGibXN4DhPappZOj6xmSFm84oKvmfKi/5R11buMAAHAiAhDs1E+Ozvj5tS2uw/LvYbE+/+8jghAAoFUiAKFBkxNilfdEoqbdENviOs5bDc18e6cGzcvSzoIyp7UNAABHNWsO0G233XbJ82VlZdq4cWOz5wAtWbJEzz33nIqKihQXF6eXX35ZI0eObLDsO++8o9///vfav3+/6urq1LdvX82cOVNTpkyxlTEMQ/PmzdPy5ctVVlam66+/XkuXLlXfvn2b1B5PnQPUmC2HSjVlxTc6V+dYPWP7d9Or9zb8XAEAcJTL5gAFBwdf8tOzZ0+lpqY2q7Fr165Venq65s2bp61btyouLk5JSUk6ceJEg+WvuOIKPf7448rNzdXOnTuVlpamtLQ0ffzxx7YyixYt0ksvvaRly5Zp06ZN6ty5s5KSknTu3LlmtQ0XjOjVVTvmJeutqT9RQIeW15P9z5O65sl/6NM9Rc5rHAAALeDUVWAtkZCQoBEjRtherGi1WhUTE6OHH35Ys2fPblIdQ4cO1YQJE/TMM8/IMAxFRUVp5syZmjVrliSpvLxc4eHhWrlype68887L1kcPUONqzlv02Z5iTftL83aW/7Eu/j56/f6faHCPEOc0DADg8drMKrDa2lrl5eUpMTHRdszb21uJiYnKzc297PWGYSg7O1v79u3TDTfcIEk6dOiQioqK7OoMDg5WQkJCo3XW1NSooqLC7oOG+fv6aPzgKK3/7Q3q5OfV4noqayy6dfFX+p+/5DmxdQAANI2pAaikpEQWi0Xh4eF2x8PDw1VU1PgwSXl5uQIDA+Xn56cJEybo5Zdf1s033yxJtuuaU2dGRobdUF5MDG81vpy+4V20bW6SFt020KF63t9ZxP5iAAC3a5OrwLp06aLt27dry5YtevbZZ5Wenq6cnJwW1zdnzhyVl5fbPkePsnS7Kfx9ffTLkT2V90SiFjqwbF5ifzEAgHv5mvnlYWFh8vHxUXFxsd3x4uJiRURENHqdt7e3+vTpI0mKj4/X3r17lZGRoRtvvNF2XXFxsSIjI+3qjI+Pb7A+f39/+fv7O3g3nqtroL9SEmKVeG2kPt1dqMfe3d3iujKz92v5F4f0xlTmBwEAXMfUHiA/Pz8NGzZM2dnZtmNWq1XZ2dm2TVabwmq1qqamRpLUq1cvRURE2NVZUVGhTZs2NatONF99EMp7IlHPThzQ4nqqay/MD2J/MQCAq5jaAyRJ6enpuueeezR8+HCNHDlSmZmZqq6uVlpamiQpNTVV0dHRysjIkHRhvs7w4cN11VVXqaamRh999JHWrFmjpUuXSpK8vLw0Y8YM/e53v1Pfvn3Vq1cvPfnkk4qKitKkSZPMuk2P0jXQX3eN6qUhPa/QHUu/0pm6li00/OpAqWJnf6hfX9dTc291bK4RAAA/ZHoASklJ0cmTJzV37lwVFRUpPj5eWVlZtknMR44ckbf3fzqqqqur9eCDD6qgoEAdO3ZU//799dprryklJcVW5tFHH1V1dbWmTp2qsrIyjR49WllZWQoICHD7/XmyAVHB2jYvSau+PKTfZ7V8bs+Krw/rzW8L9CbDYgAAJzH9PUCtEe8Bcr7CsrN6f3uBMrL+5VA911/VVa/f/xMntQoA0J405/c3AagBBCDXKSw7q9e+OaQlOYccqodhMQDAj7WZFyHC80SGdNQjyQP09n87tq3Giq8Ps8kqAKDF6AFqAD1A7lFz3qIdR8qU+mfHNlqdMChCS+4a5ryGAQDaJHqA0Cb4+/poZO8LG60+lnR1i+v5cFeRBszNYpNVAECT0QPUAHqAzOGM+UFMkgYAz0UPENokZ8wPqn93EHuLAQAuhQCEVmdErwvDYr+f1PL9xVZ8fVi953yov+WxrxsA4GIEILRK/r4++tVPYvXR/4xWpw5eLarDakgz396pa+eyWgwAYI85QA1gDlDrUnPeoi/2ndBv1mx1qJ4RPUP09rTrndQqAEBrw4sQHUQAap0qz9XpL9/kO/w26Rv6dNXq3zBRGgDaGyZBo13qEtBB/31jX72a6tg7fz7ff2Gi9Jy/7XBSywAAbQ0BCG3O2AER2jV/nOYkt/zdQZL0xpYC9WKiNAB4JIbAGsAQWNvhrL3FrokI1D9m/NRJrQIAmIE5QA4iALU9Ww6VasoKx7bUkJgoDQBtGXOA4HHq3x209FdDHKpny+Eyxc7+UJnr9zmpZQCA1ogeoAbQA9S2VZ6r09+3Fej/vbfHoXq8vaTn7his24fFOKllAABXYgjMQQSg9qG0qkZrNx/Wok++d6ge9hcDgLaBAOQgAlD7Ulh2Vqu+OqhlX+Q7VM8vhkXruV/EO6VNAADnIwA5iADUPh0uqdZbW/K1ZGN+i+vw8ZIWMSwGAK0SAchBBKD2LWvXcT3w+jaH6ggO8NWa3yRocI8Q5zQKAOAwVoEBl5A8KEq75o/TY+P6triO8nPndevir3Tfys1ObBkAwF3oAWoAPUCeo7DsrN7JO6rn1js2UfrX1/XU3FsHOqlVAICWoAcIaKLIkI6aPvZqh/cXW/H1Yd4fBABtCD1ADaAHyDNVnqvTRzuP6bF3djtUD+8PAgBzMAnaQQQgz1ZaVaMVXx5weH+xwdHBev/h0U5qFQDgchgCAxzQNdBfjyQP0LK7HNtWY+excsXO/lCPvL3dOQ0DADgNPUANoAcI9Zw1LOblJT3PsBgAuBRDYA4iAOHHSqtq9Ndvjygj618O1cOwGAC4DgHIQQQgNKaw7Kxe++aQw/ODxvbvplfvHemkVgEAJOYAAS4TGdJRjyQP0Nv//RMFdGh5Pdn/PMmyeQAwET1ADaAHCE1Rc96iHUfKlPrnb3SuruX1sL8YADgHQ2AOIgChOWrOW/TZnmJN+4tj+4sRhADAMQyBAW7k7+uj8YMv7C827YbYFtdjMaSZb+/UL5Z+5bzGAQAa1CoC0JIlSxQbG6uAgAAlJCRo8+bGN5hcvny5xowZo9DQUIWGhioxMfGi8vfee6+8vLzsPsnJya6+DXi4LgEd9Nj/udbh+UFbDpcpdvaHbLQKAC5kegBau3at0tPTNW/ePG3dulVxcXFKSkrSiRMnGiyfk5OjyZMna8OGDcrNzVVMTIzGjRunY8eO2ZVLTk5WYWGh7fPGG2+443YAjejVVTvmJWvprxx7kSITpQHAdUyfA5SQkKARI0Zo8eLFkiSr1aqYmBg9/PDDmj179mWvt1gsCg0N1eLFi5WamirpQg9QWVmZ1q1b16I2MQcIzlJ5rk6vfX1ICz9xbLd5SZoxto9m3NzPCa0CgPapzcwBqq2tVV5enhITE23HvL29lZiYqNzc3CbVcebMGdXV1emKK66wO56Tk6Pu3burX79+mjZtmkpLSxuto6amRhUVFXYfwBm6BHTQtP+6Wu89eJ06dfByqK7M7P30CAGAk5gagEpKSmSxWBQeHm53PDw8XEVFRU2q47HHHlNUVJRdiEpOTtbq1auVnZ2thQsXauPGjRo/frwsFkuDdWRkZCg4ONj2iYlhFQ6cK+7KUG2bl6T/nTLU4boys/frqjkf6m95R53QMgDwTKYOgR0/flzR0dH6+uuvNWrUKNvxRx99VBs3btSmTZsuef2CBQu0aNEi5eTkaPDgwY2WO3jwoK666ip9+umnGjt27EXna2pqVFNTY/u5oqJCMTExDIHBJZy1v5jE1hoA8ENtZggsLCxMPj4+Ki4utjteXFysiIiIS177/PPPa8GCBfrkk08uGX4kqXfv3goLC9P+/fsbPO/v76+goCC7D+AqXQI6KGVkrPKeSNTCn1/rUF31O86zYgwAmsfUAOTn56dhw4YpOzvbdsxqtSo7O9uuR+jHFi1apGeeeUZZWVkaPnz4Zb+noKBApaWlioyMdEq7AWfoGuivlIQLQejRcX0dqqt+xdicv+1wUusAoH0zfRl8enq6li9frlWrVmnv3r2aNm2aqqurlZaWJklKTU3VnDlzbOUXLlyoJ598UitWrFBsbKyKiopUVFSkqqoqSVJVVZUeeeQRffPNN8rPz1d2drYmTpyoPn36KCkpyZR7BC6la6C/HnTSROk3thQwURoAmsD0AJSSkqLnn39ec+fOVXx8vLZv366srCzbxOgjR46osLDQVn7p0qWqra3VHXfcocjISNvn+eeflyT5+Pho586duvXWW3X11Vfrvvvu07Bhw/TFF1/I39/flHsEmoKJ0gDgPqa/B6g14j1AMFvluTp9c+Ck7l/j2P5iEnuMAfAcbIbqIAIQWgtnBiFfby8tvH0QQQhAu0UAchABCK2NM5fOj+gZorenXe+EVgFA60IAchABCK1VaVWN1m4+rEVO2FqDIASgvWkz7wEC0DzOXDFWv+s8S+cBeCJ6gBpADxDagprzFu09Vq5f/e83OlPn+L/Gk0f0UMbtcU5oGQCYgyEwBxGA0JbUnLfoi30n9Js1W51SH7vOA2irCEAOIgChLXLmRGmJIASg7SEAOYgAhLastKpGn+4u1GPvEoQAeBYCkIMIQGgPnBmEOvv56I2pP9HgHiGONwwAXIQA5CACENoTZy6dJwgBaM0IQA4iAKE9Kiw7q3fyjuq59Y4HIR9vadHtbK8BoHUhADmIAIT2bMeR05q8PNcpS+cJQgBaEwKQgwhAaO+c/Q4hNlwF0BoQgBxEAIKncPY7hAhCAMxEAHIQAQiexpm7zksEIQDmIAA5iAAET+XsIOTr7aWFtw8iCAFwCwKQgwhA8HSV5+r0920F+n/v7XFanew1BsDVCEAOIgABF5RW1Shr13E9ThAC0AYQgBxEAALslVbVaNOBEj34xnan1TluQHf9KXWE0+oDAAKQgwhAQMMqz9Xpy30nNI0gBKAVIgA5iAAEXBpBCEBrRAByEAEIaBpXBKHuXfz1v/cMZ78xAM1GAHIQAQhoHlesGiMIAWguApCDCEBAy5RW1ejT3YV67N3dTquTdwkBaCoCkIMIQIBjSqtqdLikSne/uskpe41JBCEAl0cAchABCHAOZ2+6Wm/G2D6acXM/p9UHoH0gADmIAAQ4V815i0ora/Tu1gI9t/57p9VLEALwQwQgBxGAANcpLDurd/KOOjUI8XZpABIByGEEIMD1CsvOatVXB7Xsi3yn1cnKMcCzEYAcRAAC3OdwSbWydh1TxsfO6xHq4OOlBbcxYRrwNAQgBxGAAPcrLDur97cXKCPrX06tl3lCgOcgADmIAASYp7DsrD7fV+zUdwlJzBMCPAEByEEEIMB8rniposQ8IaA9a87vb283temSlixZotjYWAUEBCghIUGbN29utOzy5cs1ZswYhYaGKjQ0VImJiReVNwxDc+fOVWRkpDp27KjExER9/73z5hcAcL2ugf5KSYhV3hOJ+tPdQ5xW74nKGt26+Cv1mv2hMtfvc1q9ANoW0wPQ2rVrlZ6ernnz5mnr1q2Ki4tTUlKSTpw40WD5nJwcTZ48WRs2bFBubq5iYmI0btw4HTt2zFZm0aJFeumll7Rs2TJt2rRJnTt3VlJSks6dO+eu2wLgJF0D/TVuYJR2zR+nt+5PUEAH59RrSMrM3q/Y2R9qzt92OKdSAG2G6UNgCQkJGjFihBYvXixJslqtiomJ0cMPP6zZs2df9nqLxaLQ0FAtXrxYqampMgxDUVFRmjlzpmbNmiVJKi8vV3h4uFauXKk777zzojpqampUU1Nj+7miokIxMTEMgQGtUM15iwpPn9Xb3x7Wko35Tq2b7TaAtq3NDIHV1tYqLy9PiYmJtmPe3t5KTExUbm5uk+o4c+aM6urqdMUVV0iSDh06pKKiIrs6g4ODlZCQ0GidGRkZCg4Otn1iYvjLD2it/H19FNstUI+Mv1YbZ92oOUl9nVb3eauhmW/vpFcI8ACmBqCSkhJZLBaFh4fbHQ8PD1dRUVGT6njssccUFRVlCzz11zWnzjlz5qi8vNz2OXr0aHNvBYAJeoZ11n/fdLVyZ/+XFv78WqfW/caWAsXO/lBDnv5EOwvKnFo3APP5mt0ARyxYsEBvvvmmcnJyFBAQ0OJ6/P395e/v78SWAXCnyJCOSkmIVeK1kcrLL9XU17Y5re7TZ+p06+KvJPFOIaA9MbUHKCwsTD4+PiouLrY7XlxcrIiIiEte+/zzz2vBggX65JNPNHjwYNvx+utaUieAtu2HE6bfnfYTBTj5P/HqJ033ffwj/S2PnmKgLTM1APn5+WnYsGHKzs62HbNarcrOztaoUaMavW7RokV65plnlJWVpeHDh9ud69WrlyIiIuzqrKio0KZNmy5ZJ4D2o0tABw3p2VU75icrZ+ZPnTpPSJLqLMwVAto601eBrV27Vvfcc49eeeUVjRw5UpmZmXrrrbf0z3/+U+Hh4UpNTVV0dLQyMjIkSQsXLtTcuXP1l7/8Rddff72tnsDAQAUGBtrKLFiwQKtWrVKvXr305JNPaufOndqzZ0+Thsp4ESLQ/hSWndWuo6c19XXnDY/9ECvIAPM15/e36XOAUlJSdPLkSc2dO1dFRUWKj49XVlaWbRLzkSNH5O39n46qpUuXqra2VnfccYddPfPmzdP8+fMlSY8++qiqq6s1depUlZWVafTo0crKynJonhCAti0ypKMiQzpq1/xu2l9cobv+d5PO1Dnvv//qV5DNfHunxg3orj+ljnBa3QCcz/QeoNaIHiCg/as5b1FpZY22HT6t6W9ud9n3MHEacB/2AnMQAQjwLJXn6lRScU5/zTvi9Jcr1uvg46UFtzFEBrgSAchBBCDAcx0uqdbn+4r15N/3uuw7Qjt10Kpfj2RDVsDJCEAOIgABKK2q0f6iSt2zapPO1bnueyaP6KGM2+Nc9wWAByEAOYgABKBe/d5j3xw4qdnr9rjse1hFBjiOAOQgAhCAhpRW1ehwSZXuftW5K8h+jPlCQMsQgBxEAAJwKe5aQSaJJfVAMxCAHEQAAtBU9SvIPtx5XM9/ut+l38V8IeDSCEAOIgABaInCsrM6fLJK967e7NKJ0xJhCGgIAchBBCAAjnDXxOl6hCHgAgKQgwhAAJyltKpGZ2vOK2ffCT3xvuvDEG+ehicjADmIAATAFUqralReXau/bXXdG6d/iJ4heBoCkIMIQABc7XBJtXYdPa2H1u5wy/cRhuAJCEAOIgABcJf6VWSbDpa4Zb6QRBhC+0UAchABCIAZ6ucLvbf9mJ5b/71bvpP3DKE9IQA5iAAEwGyFZWd15lyd3tl21C3zhSTeQI22jwDkIAIQgNbkcEm16urO693tBW4LQ+xNhraIAOQgAhCA1upwSbVOVpxV6p83u3Q/sh9jeT3aAgKQgwhAAFq7+v3IjpRUu+XN0z/EvCG0VgQgBxGAALQl9W+eLqk85/aeIS9J/5feIbQSBCAHEYAAtFU/7Bn69eotbg1DEr1DMBcByEEEIADtQX0YOltzXu9sPaoln+e79fvpHYK7EYAcRAAC0B4dLqmWl2HV33cWuu09Qz/EMnu4GgHIQQQgAO1d/XuGtuSXuu0N1D/GG6nhbAQgBxGAAHgSd+9Y3xCGy+AMBCAHEYAAeKr6MFRw6ozuXbVZ586b0w6Gy9ASBCAHEYAA4D/L671l6P0dx/X8p/tNawvDZWgKApCDCEAAcLH6eUP7T1Tqv/+y3bR2MFyGxhCAHEQAAoBLqzxXp4oztTpXa9FHu47rhewDprWFQIR6BCAHEYAAoHlaS++QRCDyZAQgBxGAAKDlftg7tPlQiea8t9fU9rCzvecgADmIAAQAzlO/sqymzqK/5h3R0i8Om90kdrdvpwhADiIAAYDrHC6pVl3deR04WWX6cFk9AlH70Jzf395ualOjlixZotjYWAUEBCghIUGbN29utOzu3bt1++23KzY2Vl5eXsrMzLyozPz58+Xl5WX36d+/vwvvAADQHD3DOqtPZLCSBkdr1/xx+urRG5U9Y4wyJl5jWpsys/crdvaHts/U1VtMawvcw9fML1+7dq3S09O1bNkyJSQkKDMzU0lJSdq3b5+6d+9+UfkzZ86od+/e+sUvfqHf/va3jdZ77bXX6tNPP7X97Otr6m0CABrRJaCDugR0kCRdFRGkcYOiW8Vw2Sd7Tih29oe2n5lH1P6YOgSWkJCgESNGaPHixZIkq9WqmJgYPfzww5o9e/Ylr42NjdWMGTM0Y8YMu+Pz58/XunXrtH379ha3iyEwAGgdWuNwWT1eztj6NOf3t2ldI7W1tcrLy9OcOXNsx7y9vZWYmKjc3FyH6v7+++8VFRWlgIAAjRo1ShkZGbryyisbLV9TU6OamhrbzxUVFQ59PwDAOXqGdZYk9YkM1q6ru9tWl31fXKEH3thhatve2FKgN7YU2H6ml6htMS0AlZSUyGKxKDw83O54eHi4/vnPf7a43oSEBK1cuVL9+vVTYWGhnnrqKY0ZM0bfffedunTp0uA1GRkZeuqpp1r8nQAA1/vxcNmufuGtKhCdtxqa+fZOzXx7p+0YvUStV7ubHDN+/HjbPw8ePFgJCQnq2bOn3nrrLd13330NXjNnzhylp6fbfq6oqFBMDAkeAFqzSwWiPcfL9PBbu0xu4cW9RBKhqLUwLQCFhYXJx8dHxcXFdseLi4sVERHhtO8JCQnR1Vdfrf37G9/Ez9/fX/7+/k77TgCA+/04EN04IFIlFefk4yUdO3VGaWu+NW13+x/6cShi53tzmBaA/Pz8NGzYMGVnZ2vSpEmSLkyCzs7O1kMPPeS076mqqtKBAwc0ZcoUp9UJAGj9fhiIruzWRTvmJ9t2tz92+qzSVm9pFYGoznLx0BnziVzP1CGw9PR03XPPPRo+fLhGjhypzMxMVVdXKy0tTZKUmpqq6OhoZWRkSLowcXrPnj22fz527Ji2b9+uwMBA9enTR5I0a9Ys3XLLLerZs6eOHz+uefPmycfHR5MnTzbnJgEArYK/r49iuwVKujgQ1Z636r0dR/VyjvlvqZYank9EKHIu098EvXjxYj333HMqKipSfHy8XnrpJSUkJEiSbrzxRsXGxmrlypWSpPz8fPXq1euiOn76058qJydHknTnnXfq888/V2lpqbp166bRo0fr2Wef1VVXXdXkNrEMHgA8U/2mrh18vLT9yCn9z9vfmd2kS2L4zB5bYTiIAAQAkC5s7Fo/j6i2zqLXN+frz98cM7tZl+WpW3sQgBxEAAIANOZwSbW8DKvqLIa+O1qq//u3PWY3qUk8IRQRgBxEAAIANFVb7SWSpHEDuutPqSPMbobTEIAcRAACADjih71EJeXVumfN1lax4qwp2vJkawKQgwhAAABnqjlvUWlljc7WnJdhteqr/Sc076PvzW5Ws4R26qBVvx6pwT1CzG5KowhADiIAAQBcrbTqQiCqqbPIR1Z9dbBEj7+/z+xmNVtrerM1AchBBCAAgBl+PJ/o28Mlmv1e2wtFZg2jEYAcRAACALQWPw5FG/5VpGezDpjdrBZxdTAiADmIAAQAaM1+PHy2p6hcv/3rd6ppIxOtJdesQGvO7+92txs8AADtXddAfynwP5t4x0aEaOzAHio8fVbnz1vk5+Ol/ScqNO3NHa02FH2y54S+zT+l4bFXmPL99AA1gB4gAEB7UHPeYrffWV1drd7eekwrWtF7igZEBmr8wEg9PPZqh+tiCMxBBCAAQHv2wz3PztXU6esDJzT/H+bOK8pfMMHhOhgCAwAAjYoM6Sipo+3nq3tcodsTrrKbbF1dU6PP9p3UH3IOu7w94UH+ly/kZAQgAACgLgEd1CWgg92xuF7hSh3dV+XVtfL1lmpqz+tgSYX+563vVGNxzveOHxihpXcPc05lzUAAAgAAjeoa6H9h0vW/9Y0O1c5re1w0t+jrg6V6pg0tzycAAQCAZvH39VFst0C7Y9dc2U2//Enzh9GGXhni4tY2jAAEAACc4lLDaPXvLYoO8VP5OYtq6iwyvHzUM6yzKW0lAAEAAJf68XuLAgJMbMy/eZvdAAAAAHcjAAEAAI9DAAIAAB6HAAQAADwOAQgAAHgcAhAAAPA4BCAAAOBxCEAAAMDjEIAAAIDHIQABAACPw1YYDTAMQ5JUUVFhcksAAEBT1f/erv89fikEoAZUVlZKkmJiYkxuCQAAaK7KykoFBwdfsoyX0ZSY5GGsVquOHz+uLl26yMvLy6l1V1RUKCYmRkePHlVQUJBT64br8NzaJp5b28Rza5taw3MzDEOVlZWKioqSt/elZ/nQA9QAb29v9ejRw6XfERQUxL/YbRDPrW3iubVNPLe2yezndrmen3pMggYAAB6HAAQAADwOAcjN/P39NW/ePPn7+5vdFDQDz61t4rm1TTy3tqmtPTcmQQMAAI9DDxAAAPA4BCAAAOBxCEAAAMDjEIAAAIDHIQC50ZIlSxQbG6uAgAAlJCRo8+bNZjfJo82fP19eXl52n/79+9vOnzt3TtOnT1fXrl0VGBio22+/XcXFxXZ1HDlyRBMmTFCnTp3UvXt3PfLIIzp//ry7b6Vd+/zzz3XLLbcoKipKXl5eWrdund15wzA0d+5cRUZGqmPHjkpMTNT3339vV+bUqVO66667FBQUpJCQEN13332qqqqyK7Nz506NGTNGAQEBiomJ0aJFi1x9a+3a5Z7bvffee9G/f8nJyXZleG7ul5GRoREjRqhLly7q3r27Jk2apH379tmVcdbfjTk5ORo6dKj8/f3Vp08frVy50tW3Z4cA5CZr165Venq65s2bp61btyouLk5JSUk6ceKE2U3zaNdee60KCwttny+//NJ27re//a3+/ve/6+2339bGjRt1/Phx3XbbbbbzFotFEyZMUG1trb7++mutWrVKK1eu1Ny5c824lXarurpacXFxWrJkSYPnFy1apJdeeknLli3Tpk2b1LlzZyUlJencuXO2MnfddZd2796t9evX64MPPtDnn3+uqVOn2s5XVFRo3Lhx6tmzp/Ly8vTcc89p/vz5+tOf/uTy+2uvLvfcJCk5Odnu37833njD7jzPzf02btyo6dOn65tvvtH69etVV1encePGqbq62lbGGX83Hjp0SBMmTNBNN92k7du3a8aMGfrNb36jjz/+2H03a8AtRo4caUyfPt32s8ViMaKiooyMjAwTW+XZ5s2bZ8TFxTV4rqyszOjQoYPx9ttv247t3bvXkGTk5uYahmEYH330keHt7W0UFRXZyixdutQICgoyampqXNp2TyXJePfdd20/W61WIyIiwnjuuedsx8rKygx/f3/jjTfeMAzDMPbs2WNIMrZs2WIr849//MPw8vIyjh07ZhiGYfzxj380QkND7Z7bY489ZvTr18/Fd+QZfvzcDMMw7rnnHmPixImNXsNzax1OnDhhSDI2btxoGIbz/m589NFHjWuvvdbuu1JSUoykpCRX35INPUBuUFtbq7y8PCUmJtqOeXt7KzExUbm5uSa2DN9//72ioqLUu3dv3XXXXTpy5IgkKS8vT3V1dXbPrH///rryyittzyw3N1eDBg1SeHi4rUxSUpIqKiq0e/du996Ihzp06JCKiorsnlNwcLASEhLsnlNISIiGDx9uK5OYmChvb29t2rTJVuaGG26Qn5+frUxSUpL27dun06dPu+luPE9OTo66d++ufv36adq0aSotLbWd47m1DuXl5ZKkK664QpLz/m7Mzc21q6O+jDt/JxKA3KCkpEQWi8Xu/wySFB4erqKiIpNahYSEBK1cuVJZWVlaunSpDh06pDFjxqiyslJFRUXy8/NTSEiI3TU/fGZFRUUNPtP6c3C9+j/nS/27VVRUpO7du9ud9/X11RVXXMGzNFFycrJWr16t7OxsLVy4UBs3btT48eNlsVgk8dxaA6vVqhkzZuj666/XwIEDJclpfzc2VqaiokJnz551xe1chN3g4bHGjx9v++fBgwcrISFBPXv21FtvvaWOHTua2DKg/bvzzjtt/zxo0CANHjxYV111lXJycjR27FgTW4Z606dP13fffWc3N7I9oQfIDcLCwuTj43PRLPni4mJFRESY1Cr8WEhIiK6++mrt379fERERqq2tVVlZmV2ZHz6ziIiIBp9p/Tm4Xv2f86X+3YqIiLhoscH58+d16tQpnmUr0rt3b4WFhWn//v2SeG5me+ihh/TBBx9ow4YN6tGjh+24s/5ubKxMUFCQ2/4DlADkBn5+fho2bJiys7Ntx6xWq7KzszVq1CgTW4Yfqqqq0oEDBxQZGalhw4apQ4cOds9s3759OnLkiO2ZjRo1Srt27bL7S3r9+vUKCgrSgAED3N5+T9SrVy9FRETYPaeKigpt2rTJ7jmVlZUpLy/PVuazzz6T1WpVQkKCrcznn3+uuro6W5n169erX79+Cg0NddPdeLaCggKVlpYqMjJSEs/NLIZh6KGHHtK7776rzz77TL169bI776y/G0eNGmVXR30Zt/5OdNt0aw/35ptvGv7+/sbKlSuNPXv2GFOnTjVCQkLsZsnDvWbOnGnk5OQYhw4dMr766isjMTHRCAsLM06cOGEYhmE88MADxpVXXml89tlnxrfffmuMGjXKGDVqlO368+fPGwMHDjTGjRtnbN++3cjKyjK6detmzJkzx6xbapcqKyuNbdu2Gdu2bTMkGS+++KKxbds24/Dhw4ZhGMaCBQuMkJAQ47333jN27txpTJw40ejVq5dx9uxZWx3JycnGkCFDjE2bNhlffvml0bdvX2Py5Mm282VlZUZ4eLgxZcoU47vvvjPefPNNo1OnTsYrr7zi9vttLy713CorK41Zs2YZubm5xqFDh4xPP/3UGDp0qNG3b1/j3Llztjp4bu43bdo0Izg42MjJyTEKCwttnzNnztjKOOPvxoMHDxqdOnUyHnnkEWPv3r3GkiVLDB8fHyMrK8tt90oAcqOXX37ZuPLKKw0/Pz9j5MiRxjfffGN2kzxaSkqKERkZafj5+RnR0dFGSkqKsX//ftv5s2fPGg8++KARGhpqdOrUyfj5z39uFBYW2tWRn59vjB8/3ujYsaMRFhZmzJw506irq3P3rbRrGzZsMCRd9LnnnnsMw7iwFP7JJ580wsPDDX9/f2Ps2LHGvn377OooLS01Jk+ebAQGBhpBQUFGWlqaUVlZaVdmx44dxujRow1/f38jOjraWLBggbtusV261HM7c+aMMW7cOKNbt25Ghw4djJ49exr333//Rf9ByHNzv4aemSTjz3/+s62Ms/5u3LBhgxEfH2/4+fkZvXv3tvsOd/AyDMNwX38TAACA+ZgDBAAAPA4BCAAAeBwCEAAA8DgEIAAA4HEIQAAAwOMQgAAAgMchAAEAAI9DAAIAAB6HAAQADYiNjVVmZqbZzQDgIgQgAKa79957NWnSJEnSjTfeqBkzZrjtu1euXKmQkJCLjm/ZskVTp051WzsAuJev2Q0AAFeora2Vn59fi6/v1q2bE1sDoLWhBwhAq3Hvvfdq48aN+sMf/iAvLy95eXkpPz9fkvTdd99p/PjxCgwMVHh4uKZMmaKSkhLbtTfeeKMeeughzZgxQ2FhYUpKSpIkvfjiixo0aJA6d+6smJgYPfjgg6qqqpIk5eTkKC0tTeXl5bbvmz9/vqSLh8COHDmiiRMnKjAwUEFBQfrlL3+p4uJi2/n58+crPj5ea9asUWxsrIKDg3XnnXeqsrLStX9oAFqEAASg1fjDH/6gUaNG6f7771dhYaEKCwsVExOjsrIy/dd//ZeGDBmib7/9VllZWSouLtYvf/lLu+tXrVolPz8/ffXVV1q2bJkkydvbWy+99JJ2796tVatW6bPPPtOjjz4qSbruuuuUmZmpoKAg2/fNmjXronZZrVZNnDhRp06d0saNG7V+/XodPHhQKSkpduUOHDigdevW6YMPPtAHH3ygjRs3asGCBS760wLgCIbAALQawcHB8vPzU6dOnRQREWE7vnjxYg0ZMkS///3vbcdWrFihmJgY/etf/9LVV18tSerbt68WLVpkV+cP5xPFxsbqd7/7nR544AH98Y9/lJ+fn4KDg+Xl5WX3fT+WnZ2tXbt26dChQ4qJiZEkrV69Wtdee622bNmiESNGSLoQlFauXKkuXbpIkqZMmaLs7Gw9++yzjv3BAHA6eoAAtHo7duzQhg0bFBgYaPv0799f0oVel3rDhg276NpPP/1UY8eOVXR0tLp06aIpU6aotLRUZ86cafL37927VzExMbbwI0kDBgxQSEiI9u7dazsWGxtrCz+SFBkZqRMnTjTrXgG4Bz1AAFq9qqoq3XLLLVq4cOFF5yIjI23/3LlzZ7tz+fn5+tnPfqZp06bp2Wef1RVXXKEvv/xS9913n2pra9WpUyentrNDhw52P3t5eclqtTr1OwA4BwEIQKvi5+cni8Vid2zo0KH629/+ptjYWPn6Nv2vrby8PFmtVr3wwgvy9r7Q4f3WW29d9vt+7JprrtHRo0d19OhRWy/Qnj17VFZWpgEDBjS5PQBaD4bAALQqsbGx2rRpk/Lz81VSUiKr1arp06fr1KlTmjx5srZs2aIDBw7o448/Vlpa2iXDS58+fVRXV6eXX35ZBw8e1Jo1a2yTo3/4fVVVVcrOzlZJSUmDQ2OJiYkaNGiQ7rrrLm3dulWbN29WamqqfvrTn2r48OFO/zMA4HoEIACtyqxZs+Tj46MBAwaoW7duOnLkiKKiovTVV1/JYrFo3LhxGjRokGbMmKGQkBBbz05D4uLi9OKLL2rhwoUaOHCgXn/9dWVkZNiVue666/TAAw8oJSVF3bp1u2gStXRhKOu9995TaGiobrjhBiUmJqp3795au3at0+8fgHt4GYZhmN0IAAAAd6IHCAAAeBwCEAAA8DgEIAAA4HEIQAAAwOMQgAAAgMchAAEAAI9DAAIAAB6HAAQAADwOAQgAAHgcAhAAAPA4BCAAAOBx/j/GHY/tUFgmkQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loss_history \u001b[39m=\u001b[39m train_setup(net2, lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, n_batches\u001b[39m=\u001b[39;49m\u001b[39m20000\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m64\u001b[39;49m, display_freq\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, device \u001b[39m=\u001b[39;49m device_gpu)\n",
            "\u001b[1;32m/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m loss_arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(n_batches \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_batches):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss_arr[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m (loss_arr[i]\u001b[39m*\u001b[39mi \u001b[39m+\u001b[39m train_batch(net, opt, criterion, batch_size, device \u001b[39m=\u001b[39;49m device, teacher_force \u001b[39m=\u001b[39;49m i\u001b[39m<\u001b[39;49mteacher_force_upto ))\u001b[39m/\u001b[39m(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39mdisplay_freq \u001b[39m==\u001b[39m display_freq\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         clear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[1;32m/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mfor\u001b[39;00m index, output \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(outputs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         loss \u001b[39m=\u001b[39m criterion(output, gt[index]) \u001b[39m/\u001b[39m batch_size\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         loss\u001b[39m.\u001b[39;49mbackward(retain_graph \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/Eng2Hindi_DL_Transliteration.ipynb#X46sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m opt\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/Documents/ML/my_venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/ML/my_venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loss_history = train_setup(net2, lr=0.001, n_batches=20000, batch_size = 64, display_freq=10, device = device_gpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GM1Tj20omMi1"
      },
      "source": [
        "### Training with Attention Type 2 (Mitesh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lxFLBqW1Ip4v"
      },
      "outputs": [],
      "source": [
        "net3 = Transliteration_EncoderDecoderAttention_Type2(len(eng_alpha2index), 256, len(hindi_alpha2index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "colab_type": "code",
        "id": "tdRpJUXNIwuv",
        "outputId": "6ced8c1c-ed2c-48f3-cc79-2235af5171bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 10579 Loss 0.04278186336159706\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss_history = train_setup(net3, lr=0.001, n_batches=20000, batch_size = 64, display_freq=10, device = device_gpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "05F1-FwX6YVZ"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "bT8bibYl7CgX",
        "outputId": "d99f252b-beb5-4390-a2f4-927751e6166d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  76.81305912121913\n"
          ]
        }
      ],
      "source": [
        "def infer(net, word, char_limit, device = 'cpu'):\n",
        "    input = word_rep(word, eng_alpha2index, device)\n",
        "    return net(input, char_limit)\n",
        "\n",
        "def test(net, word, device = 'cpu'):\n",
        "    net = net.eval().to(device)\n",
        "    outputs = infer(net, word, 30, device)\n",
        "    hindi_output = ''\n",
        "    for out in outputs:\n",
        "        val, indices = out.topk(1)\n",
        "        index = indices.tolist()[0][0]\n",
        "        if index == 0:\n",
        "            break\n",
        "        hindi_char = hindi_alphabets[index+1]\n",
        "        hindi_output += hindi_char\n",
        "    print(word + ' - ' + hindi_output)\n",
        "    return hindi_output\n",
        "\n",
        "def calc_accuracy(net, device = 'cpu'):\n",
        "    net = net.eval().to(device)\n",
        "    predictions = []\n",
        "    accuracy = 0\n",
        "    for i in range(len(test_data)):\n",
        "        eng, hindi = test_data[i]\n",
        "        gt = gt_rep(hindi, hindi_alpha2index, device)\n",
        "        outputs = infer(net, eng, gt.shape[0], device)\n",
        "        correct = 0\n",
        "        for index, out in enumerate(outputs):\n",
        "            val, indices = out.topk(1)\n",
        "            hindi_pos = indices.tolist()[0]\n",
        "            if hindi_pos[0] == gt[index][0]:\n",
        "                correct += 1\n",
        "        \n",
        "        accuracy += correct/gt.shape[0]\n",
        "    accuracy /= len(test_data)\n",
        "    return accuracy\n",
        "\n",
        "# hindi_word = test(net2, 'HELLO')\n",
        "\n",
        "accuracy = calc_accuracy(net2) * 100\n",
        "\n",
        "print('Accuracy: ', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DIKHSHA - नुहॏघी\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'नुहॏघी'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test(net, 'DIKHSHA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "IOI4rlrgxPI-",
        "eYrAa5laSptM",
        "M9iH3ZvyOeNa",
        "SSw1SMZmx9A3",
        "Ob3F9Dh4PChB",
        "7l-iaCVdx5Ez"
      ],
      "include_colab_link": true,
      "name": "Eng2Hindi - DL Transliteration.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
