{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text to numerical values\n",
    "# need a vocab mapping each word to a index\n",
    "# Pytorch dataset to load the data\n",
    "# set up a padding of every batch. \n",
    "\n",
    "# set up a dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.stoi = {'<PAD>':0, '<SOS>':1, '<EOS>':2, '<UNK>':3}\n",
    "        self.itos = {0:'<PAD>', 1: '<SOS>', 2:'<EOS>', 3:'<UNK>'}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def build_vocabulary(self):\n",
    "        hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
    "        hindi_alphabet_size = len(hindi_alphabets)\n",
    "        for index, alpha in enumerate(hindi_alphabets):\n",
    "            if alpha not in self.stoi:\n",
    "                self.stoi[alpha] = index+1\n",
    "        return self.stoi\n",
    "\n",
    "    def numericalized(self, word):\n",
    "        gt_rep = torch.zeros([len(word), 1], dtype=torch.long)\n",
    "        for letter_index, letter in enumerate(word):\n",
    "            print(letter)\n",
    "            pos = self.stoi[letter]\n",
    "            gt_rep[letter_index][0] = pos\n",
    "        #gt_rep[letter_index+1][0] = self.stoi[pad_char]\n",
    "        return gt_rep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ब\n",
      "ा\n",
      "ह\n",
      "र\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[45],\n",
       "        [63],\n",
       "        [58],\n",
       "        [49]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vb = Vocabulary(2)\n",
    "l2i = vb.build_vocabulary()\n",
    "vb.numericalized('बाहर')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickerDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_excel(captions_file)\n",
    "        print(self.df.head())\n",
    "        self.caption_file = captions_file\n",
    "        self.transform = transform\n",
    "\n",
    "        self.imgs = self.df[\"filename\"]\n",
    "        self.captions = self.df[\"word\"]\n",
    "\n",
    "        #Initialize vocab and build vocab\n",
    "\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        caption = self.caption[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalized(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return img, torch.tensor(numericalized_caption)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
    "\n",
    "        return imgs, targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_loader(\n",
    "        root_folder,\n",
    "        annotation_file,\n",
    "        transform,\n",
    "        batch_size=32,\n",
    "        num_workers=8,\n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "):\n",
    "    dataset = FlickerDataset(root_folder, annotation_file, transform=transform)\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx)\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x1         x2         x3          x4          y1          y2  \\\n",
      "0  233.63464  271.41425  267.73364  229.954040    3.323353    6.229999   \n",
      "1  192.47371  254.37694  254.17207  192.268840  260.995420  261.262630   \n",
      "2  107.56752  166.65314  165.53746  106.451836    3.107311    5.439358   \n",
      "3  385.29205  421.92517  399.32580  362.692700  115.051160  149.435040   \n",
      "4  499.93628  569.57090  564.70090  495.066280   27.582123   36.568360   \n",
      "\n",
      "           y3          y4  word       filename  \n",
      "0   54.069360   51.162712   लोग   3644_लोग.jpg  \n",
      "1  308.725280  308.458070  उनके  1053_उनके.jpg  \n",
      "2   33.706670   31.374622    यह    1735_यह.jpg  \n",
      "3  173.512770  139.128890   जेल   3122_जेल.jpg  \n",
      "4   74.306305   65.320070    भर    2228_भर.jpg  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'FlickerDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mprint\u001b[39m(captions\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     main()\n",
      "\u001b[1;32m/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb Cell 8\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     [\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         transforms\u001b[39m.\u001b[39mResize((\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m)),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     ]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dataloader \u001b[39m=\u001b[39m get_loader(\u001b[39m\"\u001b[39m\u001b[39mData/Images/\u001b[39m\u001b[39m\"\u001b[39m, annotation_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mData/path_to_output_excel_file.xlsx\u001b[39m\u001b[39m\"\u001b[39m, transform\u001b[39m=\u001b[39mdata_transform)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, (imgs, captions) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(imgs\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dipeshgyanchandani/Documents/ML/Project/NLP-Exercises/Transliteration-Indian-Languages/imgcap.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(captions\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Documents/ML/my-venv1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[0;32m~/Documents/ML/my-venv1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/ML/my-venv1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1033\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m   1041\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1042\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel \u001b[39m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[39m.\u001b[39;49mwrite(fp\u001b[39m.\u001b[39;49mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    dataloader = get_loader(\"Data/Images/\", annotation_file=\"Data/path_to_output_excel_file.xlsx\", transform=data_transform)\n",
    "    for idx, (imgs, captions) in enumerate(dataloader):\n",
    "        print(imgs.shape)\n",
    "        print(captions.shape)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
